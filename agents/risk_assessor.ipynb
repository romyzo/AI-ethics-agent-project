{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aebf214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "\n",
    "# 👩‍💻 Author    : Hyelim Jo (Adapted by Gemini AI)\n",
    "# 🎯 Purpose   : AI 윤리성 리스크 진단 에이전트 v1.0 - Risk Assessor\n",
    "# 📅 Created   : 2025-10-23\n",
    "# 📜 Note      : risk_assessor.ipynb\n",
    "\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a24e81d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- Update Log ----------------------------------\n",
    "\n",
    "# 2025-10-23 09:17 / 초기 생성 / Evidence Collector의 출력을 입력으로 받도록 구조 설계\n",
    "\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba5f7294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 라이브러리 불러오기 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SKAX\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-JIaWGMA_-py3.11\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3699: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# step1. 라이브러리 불러오기\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "# Pydantic을 사용하기 위해 langchain_core에서 Base Model 임포트 (langchain v0.2.x 이상 권장)\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✅ 라이브러리 불러오기 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "225d73de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ChatOpenAI LLM 초기화 완료!\n"
     ]
    }
   ],
   "source": [
    "# step2. LLM 초기화 및 설정 (LLM_MODEL 및 JSON_SCHEMA는 기존과 동일)\n",
    "try:\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2) \n",
    "    print(\"✅ ChatOpenAI LLM 초기화 완료!\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ ChatOpenAI 초기화 실패: {e}\")\n",
    "    llm = None\n",
    "    \n",
    "ASSESSOR_OUTPUT_SCHEMA = {\n",
    "    \"category\": \"string\",\n",
    "    \"risk_level\": \"string (High, Limited, Minimal 중 택 1)\",\n",
    "    \"assessment_summary\": \"string (평가 근거 및 핵심 이슈를 한국어로 3줄 이내 요약)\",\n",
    "    \"recommendation_focus\": \"string (Mitigation 에이전트가 집중해야 할 구체적인 개선 방향 (예: '데이터 다양성 확보', '알고리즘 설명가능성 강화'))\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ba1eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step3. 리스크 평가 프롬프트 정의 (기존과 동일)\n",
    "ASSESSMENT_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \n",
    "         \"당신은 AI 윤리 리스크를 평가하는 전문가입니다. \"\n",
    "         \"EU AI Act, OECD, UNESCO와 같은 **Baseline 근거**와 **최신 사회 이슈 근거**의 요약을 종합하여, \"\n",
    "         \"제공된 서비스에 대해 특정 리스크 카테고리(편향성, 프라이버시, 투명성 등)의 위험도를 판단하고, 결과를 JSON 형식으로만 반환하세요. \"\n",
    "         \"위험도는 **High, Limited, Minimal** 중 하나여야 합니다.\"\n",
    "         \"\\n\\n[출력 JSON 스키마]:\\n{schema}\"\n",
    "        ),\n",
    "        (\"human\", \n",
    "         \"--- 서비스 및 리스크 정보 ---\"\n",
    "         \"\\n서비스명: {service_name}\"\n",
    "         \"\\n평가 리스크 카테고리: {category}\"\n",
    "         \"\\n--- Baseline 근거 요약 ---\"\n",
    "         \"\\n{baseline_summaries}\"\n",
    "         \"\\n--- Issue 근거 요약 ---\"\n",
    "         \"\\n{issue_summaries}\"\n",
    "         \"\\n\\n위 정보를 바탕으로 {service_name}의 {category} 리스크 수준을 평가하고, 개선 권고안의 초점을 설정하세요.\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2428a0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step4. 리스크 평가 함수 정의 (State를 입력으로 받도록 변경)\n",
    "def assess_risk_and_update_state(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    State에서 증거 데이터를 읽고 리스크를 평가한 후, State에 결과를 저장합니다.\n",
    "    \"\"\"\n",
    "    if not llm:\n",
    "        print(\"❌ LLM 초기화 실패로 평가를 수행할 수 없습니다.\")\n",
    "        state['assessment_status'] = \"Error: LLM failed to initialize.\"\n",
    "        return state\n",
    "    \n",
    "    # 💡 State에서 Evidence Collector의 결과 읽기\n",
    "    evidence_data = state.get('evidence_data', {})\n",
    "    if not evidence_data:\n",
    "        print(\"❌ State에 'evidence_data'가 없습니다. 평가를 건너뜁니다.\")\n",
    "        state['assessment_status'] = \"Error: Missing evidence_data in state.\"\n",
    "        return state\n",
    "    \n",
    "    service_name = evidence_data.get('query', 'AI 서비스')\n",
    "    risk_categories = list(evidence_data['scores'].keys())\n",
    "    \n",
    "    assessed_risks = []\n",
    "    parser = JsonOutputParser(pydantic_object=None)\n",
    "    \n",
    "    print(f\"\\n⚖️ Risk Assessor 시작: {service_name}의 리스크 평가\")\n",
    "    \n",
    "    for category in risk_categories:\n",
    "        print(f\"\\n   🔎 {category.upper()} 리스크 평가 중...\")\n",
    "        \n",
    "        # 1. Baseline 근거 요약 결합\n",
    "        baseline_summaries = [\n",
    "            f\"- [Baseline] 출처: {src['source']} {src['chunk_info']}. 요약: {src['summary']}\"\n",
    "            for src in evidence_data.get('baseline_sources', []) if src.get('category') == category\n",
    "        ]\n",
    "        \n",
    "        # 2. Issue 근거 요약 결합\n",
    "        issue_summaries = [\n",
    "            f\"- [Issue] 출처: {src['source']}. 요약: {src['summary']}\"\n",
    "            for src in evidence_data.get('issue_sources', []) if src.get('category') == category\n",
    "        ]\n",
    "        \n",
    "        baseline_text = \"\\n\".join(baseline_summaries) if baseline_summaries else \"증거 없음 (법적/윤리 기준 미확인 또는 무관)\"\n",
    "        issue_text = \"\\n\".join(issue_summaries) if issue_summaries else \"증거 없음 (최신 사회적 이슈 미발견)\"\n",
    "\n",
    "        chain = ASSESSMENT_PROMPT | llm | parser\n",
    "\n",
    "        try:\n",
    "            assessment_result = chain.invoke({\n",
    "                \"schema\": json.dumps(ASSESSOR_OUTPUT_SCHEMA, indent=2, ensure_ascii=False),\n",
    "                \"service_name\": service_name,\n",
    "                \"category\": category,\n",
    "                \"baseline_summaries\": baseline_text,\n",
    "                \"issue_summaries\": issue_text\n",
    "            })\n",
    "            \n",
    "            assessment_result['category'] = category \n",
    "            assessed_risks.append(assessment_result)\n",
    "            print(f\"     ✅ 평가 완료: {category.upper()} -> {assessment_result.get('risk_level', 'Unknown')}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     ❌ 평가 실패 ({category}): {e}\")\n",
    "            assessed_risks.append({\n",
    "                \"category\": category,\n",
    "                \"risk_level\": \"Error\",\n",
    "                \"assessment_summary\": f\"평가 중 오류 발생: {e}\",\n",
    "                \"recommendation_focus\": \"평가 실패\"\n",
    "            })\n",
    "            \n",
    "    # 최종 결과 구조화\n",
    "    final_assessment = {\n",
    "        \"service_name\": service_name,\n",
    "        \"assessed_risks\": assessed_risks,\n",
    "    }\n",
    "    \n",
    "    # 💡 State에 평가 결과 저장\n",
    "    state['assessment_result'] = final_assessment\n",
    "    state['assessment_status'] = \"Success\"\n",
    "    \n",
    "    print(\"\\n✅ Risk Assessor 평가 및 State 업데이트 완료!\")\n",
    "    return state # 업데이트된 State 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3497664b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "⚖️ Risk Assessor 시작 (State 기반 테스트)...\n",
      "============================================================\n",
      "\n",
      "⚖️ Risk Assessor 시작: 이력서 분석 추천 시스템의 리스크 평가\n",
      "\n",
      "   🔎 BIAS 리스크 평가 중...\n",
      "     ✅ 평가 완료: BIAS -> High\n",
      "\n",
      "   🔎 PRIVACY 리스크 평가 중...\n",
      "     ✅ 평가 완료: PRIVACY -> Limited\n",
      "\n",
      "   🔎 TRANSPARENCY 리스크 평가 중...\n",
      "     ✅ 평가 완료: TRANSPARENCY -> Minimal\n",
      "\n",
      "✅ Risk Assessor 평가 및 State 업데이트 완료!\n",
      "\n",
      "============================================================\n",
      "📢 Risk Assessor 최종 결과 (업데이트된 State 내용)\n",
      "============================================================\n",
      "{\n",
      "  \"service_name\": \"이력서 분석 추천 시스템\",\n",
      "  \"assessed_risks\": [\n",
      "    {\n",
      "      \"category\": \"bias\",\n",
      "      \"risk_level\": \"High\",\n",
      "      \"assessment_summary\": \"이력서 분석 추천 시스템은 EU AI Act에 따라 고위험으로 분류되며, 편향성 모니터링이 필수적입니다. 최근 AI 채용 시스템의 불공정성 문제로 사회적 논란이 발생하고 있어, 기본권 침해 위험이 큽니다.\",\n",
      "      \"recommendation_focus\": \"데이터 다양성 확보\"\n",
      "    },\n",
      "    {\n",
      "      \"category\": \"privacy\",\n",
      "      \"risk_level\": \"Limited\",\n",
      "      \"assessment_summary\": \"OECD는 민감 정보 처리 시 익명화 기술 적용과 데이터 주체의 통제권 강화를 권고하고 있으나, 최신 사회적 이슈는 발견되지 않았습니다. 따라서 프라이버시 리스크는 제한적입니다.\",\n",
      "      \"recommendation_focus\": \"데이터 주체의 통제권 강화\"\n",
      "    },\n",
      "    {\n",
      "      \"category\": \"transparency\",\n",
      "      \"risk_level\": \"Minimal\",\n",
      "      \"assessment_summary\": \"현재 이력서 분석 추천 시스템에 대한 법적 및 윤리적 기준이 확인되지 않아 투명성 리스크는 최소한으로 평가됨. 최신 사회적 이슈 또한 발견되지 않아 추가적인 우려는 없음. 그러나 투명성을 높이기 위한 노력이 필요할 수 있음.\",\n",
      "      \"recommendation_focus\": \"알고리즘의 작동 방식 및 결정 과정에 대한 설명 강화\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "🔗 다음 단계: Mitigation Recommender는 state['assessment_result']를 입력으로 사용합니다.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# step5. 테스트 실행 (State 딕셔너리를 직접 생성하여 테스트)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"⚖️ Risk Assessor 시작 (State 기반 테스트)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 💡 테스트를 위해 임시 State 객체 생성\n",
    "# 이 데이터는 실제로는 service_profiler.ipynb, evidence_collector.ipynb 실행 후 state에 저장된 상태여야 함.\n",
    "test_state = {\n",
    "    # Service Profiler의 결과 (선택 사항)\n",
    "    \"service_profile\": {\n",
    "        \"service_name\": \"이력서 분석 추천 시스템\",\n",
    "        \"service_type\": \"recruitment system\", \n",
    "        \"risk_categories\": [\"bias\", \"privacy\", \"transparency\"]\n",
    "    },\n",
    "    # Evidence Collector의 결과 (필수)\n",
    "    \"evidence_data\": {\n",
    "        \"query\": \"이력서 분석 추천 시스템\",\n",
    "        \"scores\": {\"bias\": 1.0, \"privacy\": 1.0, \"transparency\": 1.0},\n",
    "        \"baseline_sources\": [\n",
    "            {\"category\": \"bias\", \"source\": \"EU_AI_Act.pdf\", \"chunk_info\": \"(페이지 10의 내용)\", \"summary\": \"EU AI Act는 채용 AI를 고위험으로 분류하며, 기본권 침해 위험을 줄이기 위한 편향성 모니터링을 의무화한다.\", \"full_content\": \"...\"},\n",
    "            {\"category\": \"privacy\", \"source\": \"OECD_Privacy_2024.pdf\", \"chunk_info\": \"(페이지 5의 내용)\", \"summary\": \"OECD는 민감 정보 처리 시 익명화 기술 적용과 데이터 주체의 통제권 강화를 권고한다.\", \"full_content\": \"...\"}\n",
    "        ],\n",
    "        \"issue_sources\": [\n",
    "            {\"category\": \"bias\", \"source\": \"AI Ethics Today\", \"url\": \"...\", \"summary\": \"최근 기사에 따르면, AI 채용 시스템이 특정 집단에게 불리하게 작동하여 사회적 논란이 되고 있으며 공정성에 대한 의문이 제기되고 있다.\", \"full_content\": \"...\"}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# 리스크 평가 실행 (State 업데이트)\n",
    "updated_state = assess_risk_and_update_state(test_state)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📢 Risk Assessor 최종 결과 (업데이트된 State 내용)\")\n",
    "print(\"=\"*60)\n",
    "# 업데이트된 state['assessment_result'] 내용 출력\n",
    "print(json.dumps(updated_state.get('assessment_result'), indent=2, ensure_ascii=False))\n",
    "\n",
    "print(f\"\\n🔗 다음 단계: Mitigation Recommender는 state['assessment_result']를 입력으로 사용합니다.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-JIaWGMA_-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
