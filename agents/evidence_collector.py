# ==============================================================================

# ğŸ‘©â€ğŸ’» Author Â  Â : Hyelim Jo
# ğŸ¯ Purpose Â  : AI ìœ¤ë¦¬ì„± ë¦¬ìŠ¤í¬ ì§„ë‹¨ ì—ì´ì „íŠ¸ v1.0
# ğŸ“… Created Â  : 2025-10-22
# ğŸ“œ Note Â  Â  Â : evidence_collector.ipynb

# ==============================================================================

# -------------------------------- Update Log ----------------------------------

# 2025-10-22 16:00 / ì´ˆê¸° ìƒì„± / Evidence Collector ê¸°ë³¸ êµ¬ì¡° êµ¬í˜„
# 2025-10-22 16:30 / RAG ë©”ëª¨ë¦¬ ì„¤ê³„ / Baseline + Issue ë©”ëª¨ë¦¬ ë¶„ë¦¬
# 2025-10-22 17:00 / HuggingFace ì„ë² ë”© ì ìš© / ê²½ì œì„± ê°œì„ 
# 2025-10-23 09:00 / ì›¹ í¬ë¡¤ë§ ì‹¤ì œ êµ¬í˜„ / Tavily Search APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì‹  ë‰´ìŠ¤/ë…¼ë¬¸ ìˆ˜ì§‘
# 2025-10-23 09:30 / Baseline ì¿¼ë¦¬ ê°•í™” / EU, OECD, UNESCO ê¸°ì¤€ ëª…ì‹œ ë° íŒŒì¼ êµ¬ì„±ì— ë§ì¶° ë¡œë“œ ë¡œì§ ëª…í™•í™”
# 2025-10-23 11:00 / í‰ê°€ ë¡œì§ êµ¬í˜„ / LLM ê¸°ë°˜ì˜ ìœ„í—˜ë„(High/Limited/Minimal) í‰ê°€
# 2025-10-23 11:30 / JSON ì¶œë ¥ í¬ë§· ì •ì˜ / Mitigation Recommenderì—ê²Œ ì „ë‹¬í•  êµ¬ì¡° í™•ì •

# ------------------------------------------------------------------------------

# step1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import os
import json
import requests
from datetime import datetime, timedelta
from typing import Dict, List, Any
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ì„ íƒì  ë¼ì´ë¸ŒëŸ¬ë¦¬ import (ì˜¤ë¥˜ ë°©ì§€)
try:
    from bs4 import BeautifulSoup
    BEAUTIFULSOUP_AVAILABLE = True
except ImportError:
    BEAUTIFULSOUP_AVAILABLE = False
    print("âš ï¸ BeautifulSoup ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì›¹ í¬ë¡¤ë§ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

try:
    from langchain_huggingface import HuggingFaceEmbeddings
    HUGGINGFACE_AVAILABLE = True
except ImportError:
    HUGGINGFACE_AVAILABLE = False
    print("âš ï¸ langchain_huggingface ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¡œì»¬ ì„ë² ë”©ì´ ì œí•œë©ë‹ˆë‹¤.")

try:
    from langchain_chroma import Chroma
    CHROMA_AVAILABLE = True
except ImportError:
    CHROMA_AVAILABLE = False
    print("âš ï¸ langchain_chroma ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë²¡í„° ì €ì¥ì†Œê°€ ì œí•œë©ë‹ˆë‹¤.")

try:
    from langchain_community.document_loaders import PyMuPDFLoader
    PDF_LOADER_AVAILABLE = True
except ImportError:
    PDF_LOADER_AVAILABLE = False
    print("âš ï¸ PyMuPDFLoader ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PDF ë¡œë”©ì´ ì œí•œë©ë‹ˆë‹¤.")

try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    TEXT_SPLITTER_AVAILABLE = True
except ImportError:
    TEXT_SPLITTER_AVAILABLE = False
    print("âš ï¸ RecursiveCharacterTextSplitter ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

try:
    from langchain.prompts import PromptTemplate
    from langchain_openai import ChatOpenAI
    from langchain_core.output_parsers import JsonOutputParser
    from langchain_core.documents import Document
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("âš ï¸ LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. LLM ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

try:
    from tavily import TavilyClient
    TAVILY_AVAILABLE = True
except ImportError:
    TAVILY_AVAILABLE = False
    print("âš ï¸ Tavily ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ì´ ì œí•œë©ë‹ˆë‹¤.")

print("âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ!")

# step2. ì„¤ì • ë° ê²½ë¡œ ì •ì˜
# ë°ì´í„° ê²½ë¡œ ì„¤ì • (agents í´ë” ë‚´ì—ì„œ ì‹¤í–‰ ê°€ì •)
base_dir = os.path.join(os.path.dirname(__file__), "..", "data")
reference_dir = os.path.join(base_dir, "reference")
reference_dir = os.path.join(base_dir, "reference")
crawled_dir = os.path.join(base_dir, "crawled")
processed_dir = os.path.join(base_dir, "processed")
baseline_embed_dir = os.path.join(base_dir, "embeddings", "baseline")
issue_embed_dir = os.path.join(base_dir, "embeddings", "issue")

# ë””ë ‰í† ë¦¬ ìƒì„±
for dir_path in [crawled_dir, processed_dir, baseline_embed_dir, issue_embed_dir]:
    os.makedirs(dir_path, exist_ok=True)

# step3. ì„ë² ë”© ëª¨ë¸ ë° LLM ì´ˆê¸°í™”
embedding_model = None
llm = None

# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (ì„ íƒì )
if HUGGINGFACE_AVAILABLE:
    try:
        embedding_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={"device": "cpu"},
            encode_kwargs={"normalize_embeddings": True}
        )
        print("âœ… HuggingFace ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!")
    except Exception as e:
        print(f"âš ï¸ HuggingFace ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        embedding_model = None

# LLM ì´ˆê¸°í™” (ì„ íƒì )
if LANGCHAIN_AVAILABLE:
    try:
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        print("âœ… ChatOpenAI LLM ì´ˆê¸°í™” ì™„ë£Œ!")
    except Exception as e:
        print(f"âš ï¸ ChatOpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        llm = None
# step4. Baseline ë©”ëª¨ë¦¬ êµ¬ì¶• (EU, OECD, UNESCO ë¬¸ì„œ)
def build_baseline_memory():
    """ê³µì‹ ë¬¸ì„œ ê¸°ë°˜ Baseline ë©”ëª¨ë¦¬ êµ¬ì¶•"""
    baseline_docs = []
    
    # PDF íŒŒì¼ ë¡œë“œ (ì„ íƒì )
    pdf_files = [
        "EU_AI_Act.pdf",
        "OECD_Privacy_2024.pdf", 
        "UNESCO_Ethics_2021.pdf"
    ]
    
    if PDF_LOADER_AVAILABLE:
        for pdf_file in pdf_files:
            pdf_path = os.path.join(reference_dir, pdf_file)
            
            if os.path.exists(pdf_path):
                try:
                    loader = PyMuPDFLoader(pdf_path)
                    docs = loader.load()
                    print(f"âœ… {pdf_file} ë¡œë“œ ì™„ë£Œ")
                    
                    # ë©”íƒ€ë°ì´í„°ì— ë¬¸ì„œ íƒ€ì… ì¶”ê°€ ë° í˜ì´ì§€ ë²ˆí˜¸ ì •ë³´ í¬í•¨
                    for doc in docs:
                        doc.metadata["document_type"] = "baseline"
                        doc.metadata["source"] = pdf_file
                        doc.metadata["page"] = doc.metadata.get("page", 0) + 1 # í˜ì´ì§€ ë²ˆí˜¸ëŠ” 1ë¶€í„° ì‹œì‘
                    baseline_docs.extend(docs)
                except Exception as e:
                    print(f"âš ï¸ {pdf_file} ë¡œë“œ ì‹¤íŒ¨: {e}")
            else:
                print(f"âš ï¸ {pdf_file} íŒŒì¼ì´ ì§€ì •ëœ ê²½ë¡œì— ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
    else:
        print("âš ï¸ PDF ë¡œë”ê°€ ì—†ì–´ì„œ Baseline ë¬¸ì„œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    if not baseline_docs:
        print("âŒ Baseline ë¬¸ì„œë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. RAGê°€ Baseline ì¦ê±°ë¥¼ ì°¾ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        if LANGCHAIN_AVAILABLE:
            split_docs = [Document(page_content="No official baseline documents loaded.", metadata={"source": "N/A", "document_type": "baseline", "page": 0})]
        else:
            split_docs = [{"page_content": "No official baseline documents loaded.", "metadata": {"source": "N/A", "document_type": "baseline", "page": 0}}]
    else:
        # í…ìŠ¤íŠ¸ ë¶„í•  (ì„ íƒì )
        if TEXT_SPLITTER_AVAILABLE and LANGCHAIN_AVAILABLE:
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=50
            )
            split_docs = text_splitter.split_documents(baseline_docs)
        else:
            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë¶„í• 
            split_docs = []
            for doc in baseline_docs:
                content = doc.page_content
                chunks = [content[i:i+500] for i in range(0, len(content), 450)]
                for i, chunk in enumerate(chunks):
                    split_docs.append({
                        "page_content": chunk,
                        "metadata": {**doc.metadata, "chunk_id": i}
                    })
    
    # ChromaDBì— ì €ì¥ (ì„ íƒì )
    if CHROMA_AVAILABLE and embedding_model and LANGCHAIN_AVAILABLE:
        try:
            baseline_vectorstore = Chroma.from_documents(
                documents=split_docs,
                embedding=embedding_model,
                persist_directory=baseline_embed_dir
            )
            print(f"âœ… Baseline ë©”ëª¨ë¦¬ êµ¬ì¶• ì™„ë£Œ ({len(split_docs)}ê°œ ì²­í¬)")
            return baseline_vectorstore
        except Exception as e:
            print(f"âš ï¸ ChromaDB ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    else:
        print("âš ï¸ ChromaDBê°€ ì—†ì–´ì„œ Baseline ë©”ëª¨ë¦¬ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.")
        # JSON íŒŒì¼ë¡œ ì €ì¥
        baseline_data = []
        for doc in split_docs:
            if isinstance(doc, dict):
                baseline_data.append(doc)
            else:
                baseline_data.append({
                    "page_content": doc.page_content,
                    "metadata": doc.metadata
                })
        
        baseline_file = os.path.join(baseline_embed_dir, "baseline_data.json")
        with open(baseline_file, 'w', encoding='utf-8') as f:
            json.dump(baseline_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Baseline ë©”ëª¨ë¦¬ êµ¬ì¶• ì™„ë£Œ ({len(split_docs)}ê°œ ì²­í¬) - JSON íŒŒì¼ë¡œ ì €ì¥")
        return {"type": "json", "data": baseline_data}

print("âœ… Baseline ë©”ëª¨ë¦¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")
# step5. ì›¹ í¬ë¡¤ë§ í•¨ìˆ˜ ì •ì˜ (Tavily ì‚¬ìš©)
def crawl_web_content(keywords: List[str]) -> List[Dict[str, Any]]:
    """Tavilyë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ì—ì„œ ìµœì‹  AI ìœ¤ë¦¬ ì´ìŠˆ ê´€ë ¨ ê¸°ì‚¬ í¬ë¡¤ë§"""
    crawled_data = []
    
    if TAVILY_AVAILABLE:
        try:
            tavily = TavilyClient(api_key=os.environ.get("TAVILY_API_KEY"))
            search_queries = []
            for keyword in keywords:
                search_queries.extend([
                    f"AI {keyword} ìœ¤ë¦¬ ì´ìŠˆ",
                    f"AI {keyword} í¸í–¥ì„± ë¬¸ì œ",
                    f"AI {keyword} ê°œì¸ì •ë³´ë³´í˜¸",
                ])
            unique_queries = list(set(search_queries))[:5] # ìµœëŒ€ 5ê°œì˜ ê³ ìœ  ì¿¼ë¦¬ë¡œ ì œí•œ
            
            for query in unique_queries:
                print(f"     - Tavily ê²€ìƒ‰ ì¤‘: {query}...")
                try:
                    results = tavily.search(
                        query=query, 
                        search_depth="advanced", 
                        max_results=5, 
                        include_raw_content=True
                    )
                    for result in results.get("results", []):
                        if result.get("content"):
                            crawled_data.append({
                                "title": result.get("title", "No Title"),
                                "content": result["content"],
                                "source": result.get("url", "Unknown Source"),
                                "url": result.get("url", ""),
                                "date": datetime.now().strftime("%Y-%m-%d"),
                                "category": "issue"
                            })
                except Exception as e:
                    print(f"âš ï¸ Tavily ê²€ìƒ‰ ì‹¤íŒ¨ ({query}): {e}")
                    continue
        except Exception as e:
            print(f"âš ï¸ Tavily ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    else:
        print("âš ï¸ Tavilyê°€ ì—†ì–´ì„œ ì›¹ í¬ë¡¤ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        # ë”ë¯¸ ë°ì´í„° ìƒì„±
        for keyword in keywords:
            crawled_data.append({
                "title": f"AI {keyword} ìœ¤ë¦¬ ì´ìŠˆ ê´€ë ¨ ê¸°ì‚¬",
                "content": f"ìµœê·¼ AI {keyword} ê´€ë ¨ ìœ¤ë¦¬ì  ë¬¸ì œê°€ ì‚¬íšŒì  ë…¼ë€ì´ ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” AI ì‹œìŠ¤í…œì˜ ê³µì •ì„±ê³¼ íˆ¬ëª…ì„±ì— ëŒ€í•œ ìš°ë ¤ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.",
                "source": "AI Ethics Today",
                "url": f"https://example.com/ai-{keyword}-ethics",
                "date": datetime.now().strftime("%Y-%m-%d"),
                "category": "issue"
            })

    # í•„í„°ë§ ë¡œì§ (300ì ì´ìƒ, ì„ ì •ì  í‘œí˜„ ì œê±°, ì¤‘ë³µ URL ì œê±°)
    filtered_data = []
    seen_urls = set()
    for item in crawled_data:
        if len(item["content"]) >= 300:
            if not any(word in item["content"].lower() for word in ["ì¶©ê²©", "í­ë¡œ", "clickbait", "ë…¼ë€ì˜", "ëŒ€ë°•"]):
                if item["url"] not in seen_urls:
                    filtered_data.append(item)
                    seen_urls.add(item["url"])

    print(f"âœ… ì›¹ í¬ë¡¤ë§ ì™„ë£Œ ({len(filtered_data)}ê°œ ë¬¸ì„œ)")
    return filtered_data

print("âœ… ì›¹ í¬ë¡¤ë§ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")

# step6. Issue ë©”ëª¨ë¦¬ êµ¬ì¶• (ì›¹ í¬ë¡¤ë§ ê²°ê³¼ë¥¼ RAGì— ì €ì¥)
def build_issue_memory(keywords: List[str]):
    """ì›¹ í¬ë¡¤ë§ ê²°ê³¼ ê¸°ë°˜ Issue ë©”ëª¨ë¦¬ (Vectorstore) êµ¬ì¶•"""
    crawled_data = crawl_web_content(keywords)
    issue_docs = []
    
    for item in crawled_data:
        if LANGCHAIN_AVAILABLE:
            doc = Document(
                page_content=f"[ì´ìŠˆ: {item['category']}] {item['title']}\n\n{item['content']}",
                metadata={
                    "document_type": "issue",
                    "source": item["source"],
                    "url": item["url"],
                    "date": item["date"],
                    "category": item["category"], # ì„ì‹œë¡œ 'issue'ë¡œ ì„¤ì •
                    "title": item["title"]
                }
            )
        else:
            doc = {
                "page_content": f"[ì´ìŠˆ: {item['category']}] {item['title']}\n\n{item['content']}",
                "metadata": {
                    "document_type": "issue",
                    "source": item["source"],
                    "url": item["url"],
                    "date": item["date"],
                    "category": item["category"],
                    "title": item["title"]
                }
            }
        issue_docs.append(doc)
    
    if issue_docs:
        # Issue ë¬¸ì„œëŠ” ChromaDBì— ì €ì¥ (ì„ íƒì )
        if CHROMA_AVAILABLE and embedding_model and LANGCHAIN_AVAILABLE:
            try:
                issue_vectorstore = Chroma.from_documents(
                    documents=issue_docs,
                    embedding=embedding_model,
                    persist_directory=issue_embed_dir
                )
                print(f"âœ… Issue ë©”ëª¨ë¦¬ êµ¬ì¶• ì™„ë£Œ ({len(issue_docs)}ê°œ ë¬¸ì„œ)")
                return issue_vectorstore
            except Exception as e:
                print(f"âš ï¸ ChromaDB ì €ì¥ ì‹¤íŒ¨: {e}")
                return None
        else:
            # JSON íŒŒì¼ë¡œ ì €ì¥
            issue_data = []
            for doc in issue_docs:
                if isinstance(doc, dict):
                    issue_data.append(doc)
                else:
                    issue_data.append({
                        "page_content": doc.page_content,
                        "metadata": doc.metadata
                    })
            
            issue_file = os.path.join(issue_embed_dir, "issue_data.json")
            with open(issue_file, 'w', encoding='utf-8') as f:
                json.dump(issue_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… Issue ë©”ëª¨ë¦¬ êµ¬ì¶• ì™„ë£Œ ({len(issue_docs)}ê°œ ë¬¸ì„œ) - JSON íŒŒì¼ë¡œ ì €ì¥")
            return {"type": "json", "data": issue_data}
    else:
        print("âš ï¸ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

print("âœ… Issue ë©”ëª¨ë¦¬ êµ¬ì¶• í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")


# ğŸ’¡ ì‹ ê·œ í•¨ìˆ˜ ì •ì˜: LLMì„ ì´ìš©í•œ ì¦ê±° ìš”ì•½
def summarize_evidence_with_llm(docs: List, query: str) -> List[Dict[str, Any]]:
    """ê²€ìƒ‰ëœ Document ëª©ë¡ì„ LLMì„ ì‚¬ìš©í•˜ì—¬ ìš”ì•½í•˜ê³  ì„¸ë¶€ ì •ë³´ì™€ ê²°í•©í•©ë‹ˆë‹¤."""
    if not llm:
        print("âš ï¸ LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        # LLM ì—†ì´ ê°„ë‹¨í•œ ìš”ì•½ ìƒì„±
        summarized_results = []
        for doc in docs:
            if isinstance(doc, dict):
                content = doc["page_content"]
                metadata = doc["metadata"]
            else:
                content = doc.page_content
                metadata = doc.metadata
            
            source = metadata.get("source", metadata.get("url", "Unknown"))
            doc_type = metadata.get("document_type", "Unknown")
            category = metadata.get("category", "N/A")

            # ë¬¸ì„œ íƒ€ì…ì— ë”°ë¥¸ ì²­í¬ ì •ë³´ ì„¤ì •
            if doc_type == "baseline":
                chunk_info = f"(í˜ì´ì§€ {metadata.get('page', 'N/A')}ì˜ ë‚´ìš©)"
                score = 0.8 # Baseline ê°€ì¤‘ì¹˜
            else: # issue
                chunk_info = "(ì›¹ ê¸°ì‚¬ ì›ë¬¸)"
                score = 0.2 # Issue ê°€ì¤‘ì¹˜

            # ê°„ë‹¨í•œ ìš”ì•½ ìƒì„±
            summary = f"{source}ì—ì„œ {category} ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. {content[:100]}..."

            summarized_results.append({
                "category": category,
                "document_type": doc_type,
                "source": source,
                "chunk_info": chunk_info,
                "score": score,
                "summary": summary, 
                "content_excerpt": content[:300] + "...",
                "full_content": content
            })
        
        return summarized_results

    summarized_results = []
    
    if LANGCHAIN_AVAILABLE:
        summary_prompt_template = """ë‹¹ì‹ ì€ AI ìœ¤ë¦¬ ë¦¬ìŠ¤í¬ ì§„ë‹¨ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ í•œêµ­ì–´ë¡œ 3ì¤„ ì´ë‚´ì˜ ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸ ìš”ì•½ì„ ì œê³µí•˜ì„¸ìš”.
        ì´ ìš”ì•½ì€ 'AI ì„œë¹„ìŠ¤ {query}ì˜ ìœ¤ë¦¬ ë¦¬ìŠ¤í¬'ì— ëŒ€í•œ ê·¼ê±°ë¡œ ì‚¬ìš©ë  ê²ƒì…ë‹ˆë‹¤.
        ---
        ë¬¸ì„œ ì¶œì²˜: {source} ({document_type}) {chunk_info}
        ë¬¸ì„œ ë‚´ìš©: {content}
        ---
        ìš”ì•½:"""
        summary_prompt = PromptTemplate(template=summary_prompt_template, input_variables=["query", "source", "document_type", "content", "chunk_info"])

    for doc in docs:
        if isinstance(doc, dict):
            content = doc["page_content"]
            metadata = doc["metadata"]
        else:
            content = doc.page_content
            metadata = doc.metadata
        
        source = metadata.get("source", metadata.get("url", "Unknown"))
        doc_type = metadata.get("document_type", "Unknown")
        category = metadata.get("category", "N/A")

        # ë¬¸ì„œ íƒ€ì…ì— ë”°ë¥¸ ì²­í¬ ì •ë³´ ì„¤ì •
        if doc_type == "baseline":
            chunk_info = f"(í˜ì´ì§€ {metadata.get('page', 'N/A')}ì˜ ë‚´ìš©)"
            score = 0.8 # Baseline ê°€ì¤‘ì¹˜
        else: # issue
            chunk_info = "(ì›¹ ê¸°ì‚¬ ì›ë¬¸)"
            score = 0.2 # Issue ê°€ì¤‘ì¹˜

        if LANGCHAIN_AVAILABLE:
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° ìš”ì•½ ìƒì„±
            prompt_value = summary_prompt.invoke({
                "query": query,
                "source": source,
                "document_type": doc_type,
                "content": content,
                "chunk_info": chunk_info
            })
            
            try:
                # LLM í˜¸ì¶œ
                summary_response = llm.invoke(prompt_value.to_string())
                summary = summary_response.content.strip()
            except Exception as e:
                summary = f"LLM ìš”ì•½ ì‹¤íŒ¨. Error: {e}"
        else:
            # ê°„ë‹¨í•œ ìš”ì•½ ìƒì„±
            summary = f"{source}ì—ì„œ {category} ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. {content[:100]}..."
        
        # Risk Assessor ì—ì´ì „íŠ¸ì— ì „ë‹¬í•  ìƒì„¸ êµ¬ì¡°
        summarized_results.append({
            "category": category,
            "document_type": doc_type,
            "source": source,
            "chunk_info": chunk_info, # PDF í˜ì´ì§€ ë˜ëŠ” ì›¹ ê¸°ì‚¬ ì—¬ë¶€
            "score": score,
            "summary": summary, 
            "content_excerpt": content[:300] + "...", # ì›ë¬¸ ë‚´ìš©ì˜ ì¼ë¶€ (ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡)
            "full_content": content # Risk Assessorì—ì„œ í•„ìš”í•  ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ì „ì²´ ì›ë¬¸ë„ ì „ë‹¬
        })
        
    return summarized_results

# step7. ì¦ê±° ìˆ˜ì§‘ í•¨ìˆ˜ ì •ì˜ (ê°€ì¤‘ì¹˜ 8:2 ì ìš©)
def collect_evidence(service_profile: Dict[str, Any]) -> Dict[str, Any]:
    """
    ì„œë¹„ìŠ¤ í”„ë¡œíŒŒì¼ ê¸°ë°˜ ì¦ê±° ìˆ˜ì§‘ (Baseline 0.8 : Issue 0.2)
    - Risk Assessorì—ê²Œ ì „ë‹¬í•  ì¦ê±° ì†ŒìŠ¤ ëª©ë¡ ë° ê°€ì¤‘ì¹˜ ì ìˆ˜, ìš”ì•½ í¬í•¨
    """
    
    service_name = service_profile.get("service_name", "")
    # ğŸ’¡ Service Profilerì—ì„œ ë³€ê²½ëœ í‚¤ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
    risk_categories = service_profile.get("diagnosed_risk_categories", []) 
    service_type = service_profile.get("service_type", "")
    
    print(f"\nğŸ” ì¦ê±° ìˆ˜ì§‘ ì‹œì‘: {service_name}")
    
    # ë©”ëª¨ë¦¬ êµ¬ì¶•
    baseline_vectorstore = build_baseline_memory()
    issue_vectorstore = build_issue_memory(risk_categories)
    
    evidence_results = {
        "query": service_name,
        "weights": {"baseline": 0.8, "issue": 0.2},
        "scores": {},
        "baseline_sources": [],
        "issue_sources": []
    }
    
    all_docs_to_summarize = []
    
    # 1. ê° ë¦¬ìŠ¤í¬ ì¹´í…Œê³ ë¦¬ë³„ ì¦ê±° ê²€ìƒ‰
    for category in risk_categories:
        
        # Baseline ê²€ìƒ‰ ì¿¼ë¦¬ ê°•í™”
        baseline_query = f"{service_name} {category} ë¦¬ìŠ¤í¬ {service_type} (EU AI Act, OECD, UNESCO ìœ¤ë¦¬ ê¸°ì¤€)"
        issue_query = f"ìµœì‹  ë‰´ìŠ¤ ë…¼ë¬¸ AI {service_name} {category} ë¬¸ì œ"
        
        print(f"\n     ğŸ“Š {category.upper()} ë¦¬ìŠ¤í¬ ê²€ìƒ‰ ì¤‘...")
        
        # Baseline ê²€ìƒ‰ (ì„ íƒì )
        baseline_docs = []
        if baseline_vectorstore and hasattr(baseline_vectorstore, 'similarity_search'):
            try:
                baseline_docs = baseline_vectorstore.similarity_search(baseline_query, k=3)
            except Exception as e:
                print(f"âš ï¸ Baseline ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                baseline_docs = []
        elif isinstance(baseline_vectorstore, dict) and baseline_vectorstore.get("type") == "json":
            # JSON ë°ì´í„°ì—ì„œ ê°„ë‹¨í•œ ê²€ìƒ‰
            baseline_data = baseline_vectorstore["data"]
            for doc in baseline_data[:3]:  # ìµœëŒ€ 3ê°œ
                if category.lower() in doc["page_content"].lower() or "baseline" in doc["metadata"].get("document_type", ""):
                    baseline_docs.append(doc)
        
        # Issue ê²€ìƒ‰ (ì„ íƒì )
        issue_docs = []
        if issue_vectorstore and hasattr(issue_vectorstore, 'similarity_search'):
            try:
                issue_docs = issue_vectorstore.similarity_search(issue_query, k=2)
            except Exception as e:
                print(f"âš ï¸ Issue ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                issue_docs = []
        elif isinstance(issue_vectorstore, dict) and issue_vectorstore.get("type") == "json":
            # JSON ë°ì´í„°ì—ì„œ ê°„ë‹¨í•œ ê²€ìƒ‰
            issue_data = issue_vectorstore["data"]
            for doc in issue_data[:2]:  # ìµœëŒ€ 2ê°œ
                if category.lower() in doc["page_content"].lower() or "issue" in doc["metadata"].get("document_type", ""):
                    issue_docs.append(doc)
            
        # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ìš”ì•½ ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (ì¹´í…Œê³ ë¦¬ ë©”íƒ€ë°ì´í„° ë¶€ì—¬)
        for doc in baseline_docs:
            if isinstance(doc, dict):
                doc["metadata"] = doc.get("metadata", {})
                doc["metadata"]['category'] = category
            else:
                doc.metadata['category'] = category
            all_docs_to_summarize.append(doc)
            
        for doc in issue_docs:
            if isinstance(doc, dict):
                doc["metadata"] = doc.get("metadata", {})
                doc["metadata"]['category'] = category
            else:
                doc.metadata['category'] = category
            all_docs_to_summarize.append(doc)
            
        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ì°¸ê³ ìš©)
        baseline_weight = 0.8
        issue_weight = 0.2 if issue_docs else 0.0
        total_score = (len(baseline_docs) > 0) * baseline_weight + (len(issue_docs) > 0) * issue_weight
        evidence_results["scores"][category] = total_score
        
        print(f" - ê²€ìƒ‰ëœ Baseline ì²­í¬: {len(baseline_docs)}ê°œ")
        print(f" - ê²€ìƒ‰ëœ Issue ë¬¸ì„œ: {len(issue_docs)}ê°œ")

    print("\nğŸ“ ê²€ìƒ‰ëœ ì¦ê±°ë“¤ì„ LLMì„ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ ì¤‘...")
    
    # 2. í†µí•© ìš”ì•½ ë° ë°ì´í„° êµ¬ì¡°í™”
    summarized_evidences = summarize_evidence_with_llm(all_docs_to_summarize, service_name)
    
    # 3. ìµœì¢… ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    for evidence in summarized_evidences:
        if evidence['document_type'] == 'baseline':
            evidence_results["baseline_sources"].append(evidence)
        elif evidence['document_type'] == 'issue':
            evidence_results["issue_sources"].append(evidence)
    
    print(f"\nâœ… ì¦ê±° ìˆ˜ì§‘ ë° ìš”ì•½ ì™„ë£Œ!")
    return evidence_results

print("âœ… ì¦ê±° ìˆ˜ì§‘ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")

# step8. State ê¸°ë°˜ ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜
import sys
sys.path.append('..')
from state_manager import load_state, save_state, update_status

def evidence_collector_execute():
    """Evidence Collector ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "="*60)
    print("ğŸ” Evidence Collector ì‹œì‘...")
    print("="*60)
    
    # State ë¡œë“œ
    state = load_state()
    
    # Service Profiler ê²°ê³¼ í™•ì¸
    if state.get("status", {}).get("service_profiler") != "completed":
        print("âŒ Service Profilerê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return state
    
    service_profile = state.get("service_profile", {})
    if not service_profile:
        print("âŒ Service Profileì´ ì—†ìŠµë‹ˆë‹¤.")
        return state
    
    # ì¦ê±° ìˆ˜ì§‘ ì‹¤í–‰
    evidence_result = collect_evidence(service_profile)
    state["evidence_data"] = evidence_result
    
    # State ì €ì¥
    save_state(state)
    update_status(state, "evidence_collector", "completed")
    
    print(f"âœ… ì¦ê±° ìˆ˜ì§‘ ì™„ë£Œ - {len(evidence_result['baseline_sources'])}ê°œ Baseline + {len(evidence_result['issue_sources'])}ê°œ Issue")
    
    return state

print("âœ… Evidence Collector ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")

# í…ŒìŠ¤íŠ¸ ì½”ë“œëŠ” ì£¼ì„ ì²˜ë¦¬ë¨ - ì‹¤ì œ ì‹¤í–‰ ì‹œì—ëŠ” evidence_collector_execute() í•¨ìˆ˜ë§Œ í˜¸ì¶œ