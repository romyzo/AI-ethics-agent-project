{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe3d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "\n",
    "# ğŸ‘©â€ğŸ’» Author    : Hyelim Jo\n",
    "# ğŸ¯ Purpose   : AI ìœ¤ë¦¬ì„± ë¦¬ìŠ¤í¬ ì§„ë‹¨ ì—ì´ì „íŠ¸ v1.0\n",
    "# ğŸ“… Created   : 2025-10-22\n",
    "# ğŸ“œ Note      : evidence_collector.ipynb\n",
    "\n",
    "# ==============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d45862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- Update Log ----------------------------------\n",
    "\n",
    "# 2025-10-22 16:00 / ì´ˆê¸° ìƒì„± / Evidence Collector ê¸°ë³¸ êµ¬ì¡° êµ¬í˜„\n",
    "# 2025-10-22 16:30 / RAG ë©”ëª¨ë¦¬ ì„¤ê³„ / Baseline + Issue ë©”ëª¨ë¦¬ ë¶„ë¦¬\n",
    "# 2025-10-22 17:00 / HuggingFace ì„ë² ë”© ì ìš© / ê²½ì œì„± ê°œì„ \n",
    "\n",
    "# ------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ef9c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22844965",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# step2. ì„¤ì • ë° ê²½ë¡œ ì •ì˜\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# ë°ì´í„° ê²½ë¡œ ì„¤ì •\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m base_dir = \u001b[43mos\u001b[49m.path.join(\u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mai_ethics_agent\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m reference_dir = os.path.join(base_dir, \u001b[33m\"\u001b[39m\u001b[33mreference\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m crawled_dir = os.path.join(base_dir, \u001b[33m\"\u001b[39m\u001b[33mcrawled\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# step2. ì„¤ì • ë° ê²½ë¡œ ì •ì˜\n",
    "# ë°ì´í„° ê²½ë¡œ ì„¤ì •\n",
    "base_dir = os.path.join(\"..\", \"ai_ethics_agent\", \"data\")\n",
    "reference_dir = os.path.join(base_dir, \"reference\")\n",
    "crawled_dir = os.path.join(base_dir, \"crawled\")\n",
    "processed_dir = os.path.join(base_dir, \"processed\")\n",
    "baseline_embed_dir = os.path.join(base_dir, \"embeddings\", \"baseline\")\n",
    "issue_embed_dir = os.path.join(base_dir, \"embeddings\", \"issue\")\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "for dir_path in [crawled_dir, processed_dir, baseline_embed_dir, issue_embed_dir]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(\"âœ… ê²½ë¡œ ì„¤ì • ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decd4add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step3. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (HuggingFace - ê²½ì œì„±)\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "print(\"âœ… HuggingFace ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad70088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step4. Baseline ë©”ëª¨ë¦¬ êµ¬ì¶• (EU, OECD, UNESCO ë¬¸ì„œ)\n",
    "def build_baseline_memory():\n",
    "    \"\"\"ê³µì‹ ë¬¸ì„œ ê¸°ë°˜ Baseline ë©”ëª¨ë¦¬ êµ¬ì¶•\"\"\"\n",
    "    baseline_docs = []\n",
    "    \n",
    "    # PDF íŒŒì¼ ë¡œë“œ\n",
    "    pdf_files = [\n",
    "        \"EU_AI_Act.pdf\",\n",
    "        \"OECD_Privacy_2024.pdf\", \n",
    "        \"UNESCO_Ethics_2021.pdf\"\n",
    "    ]\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(reference_dir, pdf_file)\n",
    "        if os.path.exists(pdf_path):\n",
    "            loader = PyMuPDFLoader(pdf_path)\n",
    "            docs = loader.load()\n",
    "            # ë©”íƒ€ë°ì´í„°ì— ë¬¸ì„œ íƒ€ì… ì¶”ê°€\n",
    "            for doc in docs:\n",
    "                doc.metadata[\"document_type\"] = \"baseline\"\n",
    "                doc.metadata[\"source\"] = pdf_file\n",
    "            baseline_docs.extend(docs)\n",
    "            print(f\"âœ… {pdf_file} ë¡œë“œ ì™„ë£Œ\")\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ë¶„í• \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(baseline_docs)\n",
    "    \n",
    "    # ChromaDBì— ì €ì¥\n",
    "    baseline_vectorstore = Chroma.from_documents(\n",
    "        documents=split_docs,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=baseline_embed_dir\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Baseline ë©”ëª¨ë¦¬ êµ¬ì¶• ì™„ë£Œ ({len(split_docs)}ê°œ ì²­í¬)\")\n",
    "    return baseline_vectorstore\n",
    "\n",
    "print(\"âœ… Baseline ë©”ëª¨ë¦¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c485a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step5. ì›¹ í¬ë¡¤ë§ í•¨ìˆ˜ ì •ì˜\n",
    "def crawl_web_content(keywords: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"AI ìœ¤ë¦¬ ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤/ë…¼ë¬¸ í¬ë¡¤ë§\"\"\"\n",
    "    crawled_data = []\n",
    "    \n",
    "    # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¡°í•©\n",
    "    search_queries = []\n",
    "    for keyword in keywords:\n",
    "        search_queries.extend([\n",
    "            f\"AI {keyword} ìœ¤ë¦¬ ì´ìŠˆ\",\n",
    "            f\"AI {keyword} í¸í–¥ì„± ë¬¸ì œ\",\n",
    "            f\"AI {keyword} ê°œì¸ì •ë³´ë³´í˜¸\",\n",
    "            f\"AI {keyword} íˆ¬ëª…ì„±\"\n",
    "        ])\n",
    "    \n",
    "    # ìƒ˜í”Œ ë‰´ìŠ¤ ë°ì´í„° (ì‹¤ì œ êµ¬í˜„ ì‹œ API ì‚¬ìš©)\n",
    "    sample_news = [\n",
    "        {\n",
    "            \"title\": \"AI ì±„ìš© ì‹œìŠ¤í…œì˜ í¸í–¥ì„± ë¬¸ì œ ëŒ€ë‘\",\n",
    "            \"content\": \"ìµœê·¼ AI ê¸°ë°˜ ì±„ìš© ì‹œìŠ¤í…œì—ì„œ ì„±ë³„, ì—°ë ¹, ì¸ì¢…ì— ë”°ë¥¸ í¸í–¥ì„± ë¬¸ì œê°€ ì‚¬íšŒì  ì´ìŠˆë¡œ ëŒ€ë‘ë˜ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ì´ë ¥ì„œ ë¶„ì„ AIê°€ íŠ¹ì • ì§‘ë‹¨ì— ë¶ˆë¦¬í•˜ê²Œ ì‘ë™í•œë‹¤ëŠ” ì—°êµ¬ ê²°ê³¼ê°€ ë°œí‘œë˜ì—ˆìŠµë‹ˆë‹¤.\",\n",
    "            \"source\": \"AI Ethics Today\",\n",
    "            \"url\": \"https://example.com/ai-bias-hiring\",\n",
    "            \"date\": \"2024-10-20\",\n",
    "            \"category\": \"bias\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"ê°œì¸ì •ë³´ë³´í˜¸ë²• ê°œì •, AI ì„œë¹„ìŠ¤ ê·œì œ ê°•í™”\",\n",
    "            \"content\": \"ê°œì¸ì •ë³´ë³´í˜¸ë²• ê°œì •ì•ˆì´ í†µê³¼ë˜ì–´ AI ì„œë¹„ìŠ¤ì˜ ë°ì´í„° ìˆ˜ì§‘ ë° í™œìš©ì— ëŒ€í•œ ê·œì œê°€ ê°•í™”ë˜ì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ ê°œì¸ì •ë³´ ìë™ì²˜ë¦¬ ì‹œìŠ¤í…œì— ëŒ€í•œ ì‚¬ì „ ì‹¬ì˜ ì ˆì°¨ê°€ ë„ì…ë˜ì—ˆìŠµë‹ˆë‹¤.\",\n",
    "            \"source\": \"Privacy Weekly\",\n",
    "            \"url\": \"https://example.com/privacy-law-update\",\n",
    "            \"date\": \"2024-10-18\",\n",
    "            \"category\": \"privacy\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"AI ì•Œê³ ë¦¬ì¦˜ íˆ¬ëª…ì„± í™•ë³´ ë°©ì•ˆ ë…¼ì˜\",\n",
    "            \"content\": \"AI ì•Œê³ ë¦¬ì¦˜ì˜ íˆ¬ëª…ì„± í™•ë³´ë¥¼ ìœ„í•œ ì •ì±… ë…¼ì˜ê°€ í™œë°œíˆ ì§„í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ì˜ì‚¬ê²°ì • ê³¼ì •ì˜ ì„¤ëª… ê°€ëŠ¥ì„±ì„ ë†’ì´ëŠ” ë°©ì•ˆë“¤ì´ ì œì‹œë˜ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "            \"source\": \"Tech Policy News\",\n",
    "            \"url\": \"https://example.com/ai-transparency\",\n",
    "            \"date\": \"2024-10-15\",\n",
    "            \"category\": \"transparency\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # í•„í„°ë§: 1ë…„ ì´ë‚´, 300ì ì´ìƒ, ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜\n",
    "    filtered_data = []\n",
    "    cutoff_date = datetime.now() - timedelta(days=365)\n",
    "    \n",
    "    for news in sample_news:\n",
    "        # ë‚ ì§œ í•„í„°ë§ (1ë…„ ì´ë‚´)\n",
    "        news_date = datetime.strptime(news[\"date\"], \"%Y-%m-%d\")\n",
    "        if news_date >= cutoff_date:\n",
    "            # ê¸¸ì´ í•„í„°ë§ (300ì ì´ìƒ)\n",
    "            if len(news[\"content\"]) >= 300:\n",
    "                # ì„ ì •ì  í‘œí˜„ í•„í„°ë§\n",
    "                if not any(word in news[\"content\"] for word in [\"ì¶©ê²©\", \"í­ë¡œ\", \"clickbait\"]):\n",
    "                    filtered_data.append(news)\n",
    "    \n",
    "    print(f\"âœ… ì›¹ í¬ë¡¤ë§ ì™„ë£Œ ({len(filtered_data)}ê°œ ë¬¸ì„œ)\")\n",
    "    return filtered_data\n",
    "\n",
    "print(\"âœ… ì›¹ í¬ë¡¤ë§ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e403121",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# step6. Issue ë©”ëª¨ë¦¬ êµ¬ì¶• (ì›¹ í¬ë¡¤ë§ ê²°ê³¼ë¥¼ RAGì— ì €ì¥)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_issue_memory\u001b[39m(keywords: \u001b[43mList\u001b[49m[\u001b[38;5;28mstr\u001b[39m]):\n\u001b[32m      3\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"ì›¹ í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ChromaDBì— ì €ì¥í•˜ì—¬ ì¥ê¸° ë©”ëª¨ë¦¬ êµ¬ì¶•\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# ì›¹ í¬ë¡¤ë§ ì‹¤í–‰\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "# step6. Issue ë©”ëª¨ë¦¬ êµ¬ì¶• (ì›¹ í¬ë¡¤ë§ ê²°ê³¼ë¥¼ RAGì— ì €ì¥)\n",
    "def build_issue_memory(keywords: List[str]):\n",
    "    \"\"\"ì›¹ í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ChromaDBì— ì €ì¥í•˜ì—¬ ì¥ê¸° ë©”ëª¨ë¦¬ êµ¬ì¶•\"\"\"\n",
    "    \n",
    "    # ì›¹ í¬ë¡¤ë§ ì‹¤í–‰\n",
    "    crawled_data = crawl_web_content(keywords)\n",
    "    \n",
    "    # Document ê°ì²´ë¡œ ë³€í™˜\n",
    "    issue_docs = []\n",
    "    \n",
    "    for item in crawled_data:\n",
    "        doc = Document(\n",
    "            page_content=f\"{item['title']}\\n\\n{item['content']}\",\n",
    "            metadata={\n",
    "                \"document_type\": \"issue\",\n",
    "                \"source\": item[\"source\"],\n",
    "                \"url\": item[\"url\"],\n",
    "                \"date\": item[\"date\"],\n",
    "                \"category\": item[\"category\"],\n",
    "                \"title\": item[\"title\"]\n",
    "            }\n",
    "        )\n",
    "        issue_docs.append(doc)\n",
    "    \n",
    "    # ChromaDBì— ì €ì¥ (ì¥ê¸° ë©”ëª¨ë¦¬)\n",
    "    if issue_docs:\n",
    "        issue_vectorstore = Chroma.from_documents(\n",
    "            documents=issue_docs,\n",
    "            embedding=embedding_model,\n",
    "            persist_directory=issue_embed_dir\n",
    "        )\n",
    "        print(f\"âœ… Issue ë©”ëª¨ë¦¬ êµ¬ì¶• ì™„ë£Œ ({len(issue_docs)}ê°œ ë¬¸ì„œ)\")\n",
    "        return issue_vectorstore\n",
    "    else:\n",
    "        print(\"âš ï¸ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "\n",
    "print(\"âœ… Issue ë©”ëª¨ë¦¬ êµ¬ì¶• í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a131f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step7. ì¦ê±° ìˆ˜ì§‘ í•¨ìˆ˜ ì •ì˜ (ê°€ì¤‘ì¹˜ 8:2 ì ìš©)\n",
    "def collect_evidence(service_profile: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"ì„œë¹„ìŠ¤ í”„ë¡œíŒŒì¼ ê¸°ë°˜ ì¦ê±° ìˆ˜ì§‘ (Baseline 0.8 : Issue 0.2)\"\"\"\n",
    "    \n",
    "    service_name = service_profile.get(\"service_name\", \"\")\n",
    "    risk_categories = service_profile.get(\"risk_categories\", [])\n",
    "    service_type = service_profile.get(\"service_type\", \"\")\n",
    "    \n",
    "    print(f\"\\nğŸ” ì¦ê±° ìˆ˜ì§‘ ì‹œì‘: {service_name}\")\n",
    "    print(f\"   - ì„œë¹„ìŠ¤ ìœ í˜•: {service_type}\")\n",
    "    print(f\"   - ë¦¬ìŠ¤í¬ ì¹´í…Œê³ ë¦¬: {risk_categories}\")\n",
    "    \n",
    "    # Baseline ë©”ëª¨ë¦¬ êµ¬ì¶• (ê³µì‹ ë¬¸ì„œ)\n",
    "    baseline_vectorstore = build_baseline_memory()\n",
    "    \n",
    "    # Issue ë©”ëª¨ë¦¬ êµ¬ì¶• (ì›¹ í¬ë¡¤ë§ ê²°ê³¼)\n",
    "    issue_vectorstore = build_issue_memory(risk_categories)\n",
    "    \n",
    "    evidence_results = {\n",
    "        \"query\": service_name,\n",
    "        \"weights\": {\"baseline\": 0.8, \"issue\": 0.2},\n",
    "        \"scores\": {},\n",
    "        \"baseline_sources\": [],\n",
    "        \"issue_sources\": []\n",
    "    }\n",
    "    \n",
    "    # ê° ë¦¬ìŠ¤í¬ ì¹´í…Œê³ ë¦¬ë³„ ì¦ê±° ìˆ˜ì§‘\n",
    "    for category in risk_categories:\n",
    "        query = f\"{service_name} {category} ë¦¬ìŠ¤í¬ {service_type}\"\n",
    "        \n",
    "        print(f\"\\n   ğŸ“Š {category.upper()} ë¦¬ìŠ¤í¬ ë¶„ì„ ì¤‘...\")\n",
    "        \n",
    "        # Baseline ê²€ìƒ‰ (EU, OECD, UNESCO ë¬¸ì„œ)\n",
    "        baseline_docs = baseline_vectorstore.similarity_search(query, k=3)\n",
    "        baseline_score = 0.8  # ê³µì‹ ë¬¸ì„œ ê°€ì¤‘ì¹˜\n",
    "        \n",
    "        # Issue ê²€ìƒ‰ (ì›¹ í¬ë¡¤ë§ ê²°ê³¼)\n",
    "        issue_score = 0.0\n",
    "        if issue_vectorstore:\n",
    "            issue_docs = issue_vectorstore.similarity_search(query, k=2)\n",
    "            issue_score = 0.2  # ì›¹ í¬ë¡¤ë§ ê°€ì¤‘ì¹˜\n",
    "        else:\n",
    "            issue_docs = []\n",
    "        \n",
    "        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ì¹˜ ì ìš©)\n",
    "        total_score = baseline_score + issue_score\n",
    "        \n",
    "        evidence_results[\"scores\"][category] = total_score\n",
    "        \n",
    "        # Baseline ì†ŒìŠ¤ ì¶”ê°€ (ê³µì‹ ë¬¸ì„œ)\n",
    "        for doc in baseline_docs:\n",
    "            evidence_results[\"baseline_sources\"].append({\n",
    "                \"content\": doc.page_content[:300] + \"...\",\n",
    "                \"source\": doc.metadata.get(\"source\", \"Unknown\"),\n",
    "                \"category\": category,\n",
    "                \"score\": baseline_score,\n",
    "                \"document_type\": \"baseline\"\n",
    "            })\n",
    "        \n",
    "        # Issue ì†ŒìŠ¤ ì¶”ê°€ (ì›¹ í¬ë¡¤ë§)\n",
    "        for doc in issue_docs:\n",
    "            evidence_results[\"issue_sources\"].append({\n",
    "                \"content\": doc.page_content[:300] + \"...\",\n",
    "                \"source\": doc.metadata.get(\"source\", \"Unknown\"),\n",
    "                \"category\": category,\n",
    "                \"score\": issue_score,\n",
    "                \"url\": doc.metadata.get(\"url\", \"\"),\n",
    "                \"date\": doc.metadata.get(\"date\", \"\"),\n",
    "                \"document_type\": \"issue\"\n",
    "            })\n",
    "        \n",
    "        print(f\"      - Baseline ì†ŒìŠ¤: {len(baseline_docs)}ê°œ\")\n",
    "        print(f\"      - Issue ì†ŒìŠ¤: {len(issue_docs)}ê°œ\")\n",
    "        print(f\"      - ì¢…í•© ì ìˆ˜: {total_score:.1f}\")\n",
    "    \n",
    "    print(f\"\\nâœ… ì¦ê±° ìˆ˜ì§‘ ì™„ë£Œ!\")\n",
    "    return evidence_results\n",
    "\n",
    "print(\"âœ… ì¦ê±° ìˆ˜ì§‘ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c0121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step8. í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ” Evidence Collector ì‹œì‘...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Service Profilerì—ì„œ ë°›ì€ ê²°ê³¼ (ì˜ˆì‹œ)\n",
    "test_service_profile = {\n",
    "    \"service_name\": \"ì´ë ¥ì„œ ë¶„ì„ ì¶”ì²œ ì‹œìŠ¤í…œ\",\n",
    "    \"service_type\": \"recommendation\", \n",
    "    \"description\": \"ì±„ìš© ì§€ì›ìì˜ ì´ë ¥ì„œë¥¼ AIë¡œ ë¶„ì„í•˜ì—¬ ì í•©í•œ í›„ë³´ìë¥¼ ì¶”ì²œí•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\",\n",
    "    \"risk_categories\": [\"bias\", \"privacy\", \"transparency\"]\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“ ë¶„ì„í•  ì„œë¹„ìŠ¤: {test_service_profile['description']}...\")\n",
    "\n",
    "# ì¦ê±° ìˆ˜ì§‘ ì‹¤í–‰\n",
    "evidence_result = collect_evidence(test_service_profile)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š ì¦ê±° ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   - ì„œë¹„ìŠ¤ëª…: {evidence_result['query']}\")\n",
    "print(f\"   - ê°€ì¤‘ì¹˜: Baseline {evidence_result['weights']['baseline']} : Issue {evidence_result['weights']['issue']}\")\n",
    "print(f\"   - ìˆ˜ì§‘ëœ ë¦¬ìŠ¤í¬: {list(evidence_result['scores'].keys())}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ ê° ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜:\")\n",
    "for category, score in evidence_result['scores'].items():\n",
    "    print(f\"   - {category.upper()}: {score:.1f}\")\n",
    "\n",
    "print(f\"\\nğŸ“š ìˆ˜ì§‘ëœ ì¦ê±°:\")\n",
    "print(f\"   - Baseline ì†ŒìŠ¤ (ê³µì‹ë¬¸ì„œ): {len(evidence_result['baseline_sources'])}ê°œ\")\n",
    "print(f\"   - Issue ì†ŒìŠ¤ (ì›¹í¬ë¡¤ë§): {len(evidence_result['issue_sources'])}ê°œ\")\n",
    "\n",
    "print(f\"\\nğŸ”— ë‹¤ìŒ ë‹¨ê³„: Risk Assessorë¡œ ì¦ê±° ì „ë‹¬ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-JIaWGMA_-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
