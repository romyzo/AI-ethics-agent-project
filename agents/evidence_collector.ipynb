{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe3d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "\n",
    "# 👩‍💻 Author    : Hyelim Jo\n",
    "# 🎯 Purpose   : AI 윤리성 리스크 진단 에이전트 v1.0\n",
    "# 📅 Created   : 2025-10-22\n",
    "# 📜 Note      : evidence_collector.ipynb\n",
    "\n",
    "# ==============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d45862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- Update Log ----------------------------------\n",
    "\n",
    "# 2025-10-22 16:00 / 초기 생성 / Evidence Collector 기본 구조 구현\n",
    "# 2025-10-22 16:30 / RAG 메모리 설계 / Baseline + Issue 메모리 분리\n",
    "# 2025-10-22 17:00 / HuggingFace 임베딩 적용 / 경제성 개선\n",
    "\n",
    "# ------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ef9c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1. 라이브러리 불러오기\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✅ 라이브러리 불러오기 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22844965",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# step2. 설정 및 경로 정의\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 데이터 경로 설정\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m base_dir = \u001b[43mos\u001b[49m.path.join(\u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mai_ethics_agent\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m reference_dir = os.path.join(base_dir, \u001b[33m\"\u001b[39m\u001b[33mreference\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m crawled_dir = os.path.join(base_dir, \u001b[33m\"\u001b[39m\u001b[33mcrawled\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# step2. 설정 및 경로 정의\n",
    "# 데이터 경로 설정\n",
    "base_dir = os.path.join(\"..\", \"ai_ethics_agent\", \"data\")\n",
    "reference_dir = os.path.join(base_dir, \"reference\")\n",
    "crawled_dir = os.path.join(base_dir, \"crawled\")\n",
    "processed_dir = os.path.join(base_dir, \"processed\")\n",
    "baseline_embed_dir = os.path.join(base_dir, \"embeddings\", \"baseline\")\n",
    "issue_embed_dir = os.path.join(base_dir, \"embeddings\", \"issue\")\n",
    "\n",
    "# 디렉토리 생성\n",
    "for dir_path in [crawled_dir, processed_dir, baseline_embed_dir, issue_embed_dir]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(\"✅ 경로 설정 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decd4add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step3. 임베딩 모델 초기화 (HuggingFace - 경제성)\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "print(\"✅ HuggingFace 임베딩 모델 초기화 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad70088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step4. Baseline 메모리 구축 (EU, OECD, UNESCO 문서)\n",
    "def build_baseline_memory():\n",
    "    \"\"\"공식 문서 기반 Baseline 메모리 구축\"\"\"\n",
    "    baseline_docs = []\n",
    "    \n",
    "    # PDF 파일 로드\n",
    "    pdf_files = [\n",
    "        \"EU_AI_Act.pdf\",\n",
    "        \"OECD_Privacy_2024.pdf\", \n",
    "        \"UNESCO_Ethics_2021.pdf\"\n",
    "    ]\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(reference_dir, pdf_file)\n",
    "        if os.path.exists(pdf_path):\n",
    "            loader = PyMuPDFLoader(pdf_path)\n",
    "            docs = loader.load()\n",
    "            # 메타데이터에 문서 타입 추가\n",
    "            for doc in docs:\n",
    "                doc.metadata[\"document_type\"] = \"baseline\"\n",
    "                doc.metadata[\"source\"] = pdf_file\n",
    "            baseline_docs.extend(docs)\n",
    "            print(f\"✅ {pdf_file} 로드 완료\")\n",
    "    \n",
    "    # 텍스트 분할\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(baseline_docs)\n",
    "    \n",
    "    # ChromaDB에 저장\n",
    "    baseline_vectorstore = Chroma.from_documents(\n",
    "        documents=split_docs,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=baseline_embed_dir\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Baseline 메모리 구축 완료 ({len(split_docs)}개 청크)\")\n",
    "    return baseline_vectorstore\n",
    "\n",
    "print(\"✅ Baseline 메모리 함수 정의 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c485a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step5. 웹 크롤링 함수 정의\n",
    "def crawl_web_content(keywords: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"AI 윤리 관련 최신 뉴스/논문 크롤링\"\"\"\n",
    "    crawled_data = []\n",
    "    \n",
    "    # 검색 키워드 조합\n",
    "    search_queries = []\n",
    "    for keyword in keywords:\n",
    "        search_queries.extend([\n",
    "            f\"AI {keyword} 윤리 이슈\",\n",
    "            f\"AI {keyword} 편향성 문제\",\n",
    "            f\"AI {keyword} 개인정보보호\",\n",
    "            f\"AI {keyword} 투명성\"\n",
    "        ])\n",
    "    \n",
    "    # 샘플 뉴스 데이터 (실제 구현 시 API 사용)\n",
    "    sample_news = [\n",
    "        {\n",
    "            \"title\": \"AI 채용 시스템의 편향성 문제 대두\",\n",
    "            \"content\": \"최근 AI 기반 채용 시스템에서 성별, 연령, 인종에 따른 편향성 문제가 사회적 이슈로 대두되고 있습니다. 특히 이력서 분석 AI가 특정 집단에 불리하게 작동한다는 연구 결과가 발표되었습니다.\",\n",
    "            \"source\": \"AI Ethics Today\",\n",
    "            \"url\": \"https://example.com/ai-bias-hiring\",\n",
    "            \"date\": \"2024-10-20\",\n",
    "            \"category\": \"bias\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"개인정보보호법 개정, AI 서비스 규제 강화\",\n",
    "            \"content\": \"개인정보보호법 개정안이 통과되어 AI 서비스의 데이터 수집 및 활용에 대한 규제가 강화되었습니다. 특히 개인정보 자동처리 시스템에 대한 사전 심의 절차가 도입되었습니다.\",\n",
    "            \"source\": \"Privacy Weekly\",\n",
    "            \"url\": \"https://example.com/privacy-law-update\",\n",
    "            \"date\": \"2024-10-18\",\n",
    "            \"category\": \"privacy\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"AI 알고리즘 투명성 확보 방안 논의\",\n",
    "            \"content\": \"AI 알고리즘의 투명성 확보를 위한 정책 논의가 활발히 진행되고 있습니다. 특히 의사결정 과정의 설명 가능성을 높이는 방안들이 제시되고 있습니다.\",\n",
    "            \"source\": \"Tech Policy News\",\n",
    "            \"url\": \"https://example.com/ai-transparency\",\n",
    "            \"date\": \"2024-10-15\",\n",
    "            \"category\": \"transparency\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # 필터링: 1년 이내, 300자 이상, 신뢰할 수 있는 출처\n",
    "    filtered_data = []\n",
    "    cutoff_date = datetime.now() - timedelta(days=365)\n",
    "    \n",
    "    for news in sample_news:\n",
    "        # 날짜 필터링 (1년 이내)\n",
    "        news_date = datetime.strptime(news[\"date\"], \"%Y-%m-%d\")\n",
    "        if news_date >= cutoff_date:\n",
    "            # 길이 필터링 (300자 이상)\n",
    "            if len(news[\"content\"]) >= 300:\n",
    "                # 선정적 표현 필터링\n",
    "                if not any(word in news[\"content\"] for word in [\"충격\", \"폭로\", \"clickbait\"]):\n",
    "                    filtered_data.append(news)\n",
    "    \n",
    "    print(f\"✅ 웹 크롤링 완료 ({len(filtered_data)}개 문서)\")\n",
    "    return filtered_data\n",
    "\n",
    "print(\"✅ 웹 크롤링 함수 정의 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e403121",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# step6. Issue 메모리 구축 (웹 크롤링 결과를 RAG에 저장)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_issue_memory\u001b[39m(keywords: \u001b[43mList\u001b[49m[\u001b[38;5;28mstr\u001b[39m]):\n\u001b[32m      3\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"웹 크롤링 결과를 ChromaDB에 저장하여 장기 메모리 구축\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# 웹 크롤링 실행\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "# step6. Issue 메모리 구축 (웹 크롤링 결과를 RAG에 저장)\n",
    "def build_issue_memory(keywords: List[str]):\n",
    "    \"\"\"웹 크롤링 결과를 ChromaDB에 저장하여 장기 메모리 구축\"\"\"\n",
    "    \n",
    "    # 웹 크롤링 실행\n",
    "    crawled_data = crawl_web_content(keywords)\n",
    "    \n",
    "    # Document 객체로 변환\n",
    "    issue_docs = []\n",
    "    \n",
    "    for item in crawled_data:\n",
    "        doc = Document(\n",
    "            page_content=f\"{item['title']}\\n\\n{item['content']}\",\n",
    "            metadata={\n",
    "                \"document_type\": \"issue\",\n",
    "                \"source\": item[\"source\"],\n",
    "                \"url\": item[\"url\"],\n",
    "                \"date\": item[\"date\"],\n",
    "                \"category\": item[\"category\"],\n",
    "                \"title\": item[\"title\"]\n",
    "            }\n",
    "        )\n",
    "        issue_docs.append(doc)\n",
    "    \n",
    "    # ChromaDB에 저장 (장기 메모리)\n",
    "    if issue_docs:\n",
    "        issue_vectorstore = Chroma.from_documents(\n",
    "            documents=issue_docs,\n",
    "            embedding=embedding_model,\n",
    "            persist_directory=issue_embed_dir\n",
    "        )\n",
    "        print(f\"✅ Issue 메모리 구축 완료 ({len(issue_docs)}개 문서)\")\n",
    "        return issue_vectorstore\n",
    "    else:\n",
    "        print(\"⚠️ 크롤링된 데이터가 없습니다.\")\n",
    "        return None\n",
    "\n",
    "print(\"✅ Issue 메모리 구축 함수 정의 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a131f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step7. 증거 수집 함수 정의 (가중치 8:2 적용)\n",
    "def collect_evidence(service_profile: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"서비스 프로파일 기반 증거 수집 (Baseline 0.8 : Issue 0.2)\"\"\"\n",
    "    \n",
    "    service_name = service_profile.get(\"service_name\", \"\")\n",
    "    risk_categories = service_profile.get(\"risk_categories\", [])\n",
    "    service_type = service_profile.get(\"service_type\", \"\")\n",
    "    \n",
    "    print(f\"\\n🔍 증거 수집 시작: {service_name}\")\n",
    "    print(f\"   - 서비스 유형: {service_type}\")\n",
    "    print(f\"   - 리스크 카테고리: {risk_categories}\")\n",
    "    \n",
    "    # Baseline 메모리 구축 (공식 문서)\n",
    "    baseline_vectorstore = build_baseline_memory()\n",
    "    \n",
    "    # Issue 메모리 구축 (웹 크롤링 결과)\n",
    "    issue_vectorstore = build_issue_memory(risk_categories)\n",
    "    \n",
    "    evidence_results = {\n",
    "        \"query\": service_name,\n",
    "        \"weights\": {\"baseline\": 0.8, \"issue\": 0.2},\n",
    "        \"scores\": {},\n",
    "        \"baseline_sources\": [],\n",
    "        \"issue_sources\": []\n",
    "    }\n",
    "    \n",
    "    # 각 리스크 카테고리별 증거 수집\n",
    "    for category in risk_categories:\n",
    "        query = f\"{service_name} {category} 리스크 {service_type}\"\n",
    "        \n",
    "        print(f\"\\n   📊 {category.upper()} 리스크 분석 중...\")\n",
    "        \n",
    "        # Baseline 검색 (EU, OECD, UNESCO 문서)\n",
    "        baseline_docs = baseline_vectorstore.similarity_search(query, k=3)\n",
    "        baseline_score = 0.8  # 공식 문서 가중치\n",
    "        \n",
    "        # Issue 검색 (웹 크롤링 결과)\n",
    "        issue_score = 0.0\n",
    "        if issue_vectorstore:\n",
    "            issue_docs = issue_vectorstore.similarity_search(query, k=2)\n",
    "            issue_score = 0.2  # 웹 크롤링 가중치\n",
    "        else:\n",
    "            issue_docs = []\n",
    "        \n",
    "        # 종합 점수 계산 (가중치 적용)\n",
    "        total_score = baseline_score + issue_score\n",
    "        \n",
    "        evidence_results[\"scores\"][category] = total_score\n",
    "        \n",
    "        # Baseline 소스 추가 (공식 문서)\n",
    "        for doc in baseline_docs:\n",
    "            evidence_results[\"baseline_sources\"].append({\n",
    "                \"content\": doc.page_content[:300] + \"...\",\n",
    "                \"source\": doc.metadata.get(\"source\", \"Unknown\"),\n",
    "                \"category\": category,\n",
    "                \"score\": baseline_score,\n",
    "                \"document_type\": \"baseline\"\n",
    "            })\n",
    "        \n",
    "        # Issue 소스 추가 (웹 크롤링)\n",
    "        for doc in issue_docs:\n",
    "            evidence_results[\"issue_sources\"].append({\n",
    "                \"content\": doc.page_content[:300] + \"...\",\n",
    "                \"source\": doc.metadata.get(\"source\", \"Unknown\"),\n",
    "                \"category\": category,\n",
    "                \"score\": issue_score,\n",
    "                \"url\": doc.metadata.get(\"url\", \"\"),\n",
    "                \"date\": doc.metadata.get(\"date\", \"\"),\n",
    "                \"document_type\": \"issue\"\n",
    "            })\n",
    "        \n",
    "        print(f\"      - Baseline 소스: {len(baseline_docs)}개\")\n",
    "        print(f\"      - Issue 소스: {len(issue_docs)}개\")\n",
    "        print(f\"      - 종합 점수: {total_score:.1f}\")\n",
    "    \n",
    "    print(f\"\\n✅ 증거 수집 완료!\")\n",
    "    return evidence_results\n",
    "\n",
    "print(\"✅ 증거 수집 함수 정의 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c0121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step8. 테스트 실행\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🔍 Evidence Collector 시작...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Service Profiler에서 받은 결과 (예시)\n",
    "test_service_profile = {\n",
    "    \"service_name\": \"이력서 분석 추천 시스템\",\n",
    "    \"service_type\": \"recommendation\", \n",
    "    \"description\": \"채용 지원자의 이력서를 AI로 분석하여 적합한 후보자를 추천하는 시스템입니다.\",\n",
    "    \"risk_categories\": [\"bias\", \"privacy\", \"transparency\"]\n",
    "}\n",
    "\n",
    "print(f\"\\n📝 분석할 서비스: {test_service_profile['description']}...\")\n",
    "\n",
    "# 증거 수집 실행\n",
    "evidence_result = collect_evidence(test_service_profile)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📊 증거 수집 결과 요약\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   - 서비스명: {evidence_result['query']}\")\n",
    "print(f\"   - 가중치: Baseline {evidence_result['weights']['baseline']} : Issue {evidence_result['weights']['issue']}\")\n",
    "print(f\"   - 수집된 리스크: {list(evidence_result['scores'].keys())}\")\n",
    "\n",
    "print(f\"\\n📈 각 카테고리별 점수:\")\n",
    "for category, score in evidence_result['scores'].items():\n",
    "    print(f\"   - {category.upper()}: {score:.1f}\")\n",
    "\n",
    "print(f\"\\n📚 수집된 증거:\")\n",
    "print(f\"   - Baseline 소스 (공식문서): {len(evidence_result['baseline_sources'])}개\")\n",
    "print(f\"   - Issue 소스 (웹크롤링): {len(evidence_result['issue_sources'])}개\")\n",
    "\n",
    "print(f\"\\n🔗 다음 단계: Risk Assessor로 증거 전달 준비 완료\")\n",
    "print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-JIaWGMA_-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
