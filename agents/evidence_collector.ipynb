{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fe3d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "\n",
    "# ğŸ‘©â€ğŸ’» Author Â  Â : Hyelim Jo\n",
    "# ğŸ¯ Purpose Â  : AI ìœ¤ë¦¬ì„± ë¦¬ìŠ¤í¬ ì§„ë‹¨ ì—ì´ì „íŠ¸ v1.0\n",
    "# ğŸ“… Created Â  : 2025-10-22\n",
    "# ğŸ“œ Note Â  Â  Â : evidence_collector.ipynb\n",
    "\n",
    "# ==============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17d45862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- Update Log ----------------------------------\n",
    "\n",
    "# 2025-10-22 16:00 / ì´ˆê¸° ìƒì„± / Evidence Collector ê¸°ë³¸ êµ¬ì¡° êµ¬í˜„\n",
    "# 2025-10-22 16:30 / RAG ë©”ëª¨ë¦¬ ì„¤ê³„ / Baseline + Issue ë©”ëª¨ë¦¬ ë¶„ë¦¬\n",
    "# 2025-10-22 17:00 / HuggingFace ì„ë² ë”© ì ìš© / ê²½ì œì„± ê°œì„ \n",
    "# 2025-10-23 09:00 / ì›¹ í¬ë¡¤ë§ ì‹¤ì œ êµ¬í˜„ / Tavily Search APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì‹  ë‰´ìŠ¤/ë…¼ë¬¸ ìˆ˜ì§‘\n",
    "# 2025-10-23 09:30 / Baseline ì¿¼ë¦¬ ê°•í™” / EU, OECD, UNESCO ê¸°ì¤€ ëª…ì‹œ ë° íŒŒì¼ êµ¬ì„±ì— ë§ì¶° ë¡œë“œ ë¡œì§ ëª…í™•í™”\n",
    "\n",
    "# ------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23ef9c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# step1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI # LLM ì‚¬ìš©ì„ ìœ„í•´ í•„ìš”\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22844965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê²½ë¡œ ì„¤ì • ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# step2. ì„¤ì • ë° ê²½ë¡œ ì •ì˜\n",
    "# ë°ì´í„° ê²½ë¡œ ì„¤ì • (agents í´ë” ë‚´ì—ì„œ ì‹¤í–‰ ê°€ì •)\n",
    "base_dir = os.path.join(\"..\", \"data\")\n",
    "reference_dir = os.path.join(base_dir, \"reference\")\n",
    "crawled_dir = os.path.join(base_dir, \"crawled\")\n",
    "processed_dir = os.path.join(base_dir, \"processed\")\n",
    "baseline_embed_dir = os.path.join(base_dir, \"embeddings\", \"baseline\")\n",
    "issue_embed_dir = os.path.join(base_dir, \"embeddings\", \"issue\")\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "for dir_path in [crawled_dir, processed_dir, baseline_embed_dir, issue_embed_dir]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(\"âœ… ê²½ë¡œ ì„¤ì • ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "decd4add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SKAX\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-JIaWGMA_-py3.11\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ChatOpenAI LLM ì´ˆê¸°í™” ì™„ë£Œ!\n",
      "âœ… HuggingFace ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# step3. ì„ë² ë”© ëª¨ë¸ ë° LLM ì´ˆê¸°í™”\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "# ğŸ’¡ LLM ì´ˆê¸°í™” (ìš”ì•½ ë° í‰ê°€ì— ì‚¬ìš©)\n",
    "try:\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    print(\"âœ… ChatOpenAI LLM ì´ˆê¸°í™” ì™„ë£Œ!\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ChatOpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "    llm = None\n",
    "\n",
    "print(\"âœ… HuggingFace ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ad70088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Baseline ë©”ëª¨ë¦¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# step4. Baseline ë©”ëª¨ë¦¬ êµ¬ì¶• (EU, OECD, UNESCO ë¬¸ì„œ)\n",
    "def build_baseline_memory():\n",
    "    \"\"\"ê³µì‹ ë¬¸ì„œ ê¸°ë°˜ Baseline ë©”ëª¨ë¦¬ êµ¬ì¶•\"\"\"\n",
    "    baseline_docs = []\n",
    "    \n",
    "    # PDF íŒŒì¼ ë¡œë“œ\n",
    "    pdf_files = [\n",
    "        \"EU_AI_Act.pdf\",\n",
    "        \"OECD_Privacy_2024.pdf\", \n",
    "        \"UNESCO_Ethics_2021.pdf\"\n",
    "    ]\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(reference_dir, pdf_file)\n",
    "        \n",
    "        if os.path.exists(pdf_path):\n",
    "            loader = PyMuPDFLoader(pdf_path)\n",
    "            docs = loader.load()\n",
    "            print(f\"âœ… {pdf_file} ë¡œë“œ ì™„ë£Œ\")\n",
    "            \n",
    "            # ë©”íƒ€ë°ì´í„°ì— ë¬¸ì„œ íƒ€ì… ì¶”ê°€ ë° í˜ì´ì§€ ë²ˆí˜¸ ì •ë³´ í¬í•¨\n",
    "            for doc in docs:\n",
    "                doc.metadata[\"document_type\"] = \"baseline\"\n",
    "                doc.metadata[\"source\"] = pdf_file\n",
    "                doc.metadata[\"page\"] = doc.metadata.get(\"page\", 0) + 1 # í˜ì´ì§€ ë²ˆí˜¸ëŠ” 1ë¶€í„° ì‹œì‘\n",
    "            baseline_docs.extend(docs)\n",
    "        else:\n",
    "            print(f\"âš ï¸ {pdf_file} íŒŒì¼ì´ ì§€ì •ëœ ê²½ë¡œì— ì—†ìŠµë‹ˆë‹¤: {pdf_path}\")\n",
    "    \n",
    "    if not baseline_docs:\n",
    "        print(\"âŒ Baseline ë¬¸ì„œë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. RAGê°€ Baseline ì¦ê±°ë¥¼ ì°¾ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "        split_docs = [Document(page_content=\"No official baseline documents loaded.\", metadata={\"source\": \"N/A\", \"document_type\": \"baseline\", \"page\": 0})]\n",
    "    else:\n",
    "        # í…ìŠ¤íŠ¸ ë¶„í• \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=50\n",
    "        )\n",
    "        split_docs = text_splitter.split_documents(baseline_docs)\n",
    "    \n",
    "    # ChromaDBì— ì €ì¥\n",
    "    baseline_vectorstore = Chroma.from_documents(\n",
    "        documents=split_docs,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=baseline_embed_dir\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Baseline ë©”ëª¨ë¦¬ êµ¬ì¶• ì™„ë£Œ ({len(split_docs)}ê°œ ì²­í¬)\")\n",
    "    return baseline_vectorstore\n",
    "\n",
    "print(\"âœ… Baseline ë©”ëª¨ë¦¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c485a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì›¹ í¬ë¡¤ë§ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# step5. ì›¹ í¬ë¡¤ë§ í•¨ìˆ˜ ì •ì˜ (Tavily ì‚¬ìš©)\n",
    "def crawl_web_content(keywords: List[str]) -> List[Dict[str, Any]]:\n",
    "    # ... (Tavily search logic remains the same)\n",
    "    tavily = TavilyClient(api_key=os.environ.get(\"TAVILY_API_KEY\"))\n",
    "    crawled_data = []\n",
    "    search_queries = []\n",
    "    for keyword in keywords:\n",
    "        search_queries.extend([\n",
    "            f\"AI {keyword} ìœ¤ë¦¬ ì´ìŠˆ\",\n",
    "            f\"AI {keyword} í¸í–¥ì„± ë¬¸ì œ\",\n",
    "            f\"AI {keyword} ê°œì¸ì •ë³´ë³´í˜¸\",\n",
    "        ])\n",
    "    unique_queries = list(set(search_queries))[:5]\n",
    "    \n",
    "    for query in unique_queries:\n",
    "        print(f\" Â  Â - Tavily ê²€ìƒ‰ ì¤‘: {query}...\")\n",
    "        try:\n",
    "            results = tavily.search(\n",
    "                query=query, \n",
    "                search_depth=\"advanced\", \n",
    "                max_results=5, \n",
    "                include_raw_content=True\n",
    "            )\n",
    "            for result in results.get(\"results\", []):\n",
    "                if result.get(\"content\"):\n",
    "                    crawled_data.append({\n",
    "                        \"title\": result.get(\"title\", \"No Title\"),\n",
    "                        \"content\": result[\"content\"],\n",
    "                        \"source\": result.get(\"url\", \"Unknown Source\"),\n",
    "                        \"url\": result.get(\"url\", \"\"),\n",
    "                        \"date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "                        \"category\": \"issue\"\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Tavily ê²€ìƒ‰ ì‹¤íŒ¨ ({query}): {e}\")\n",
    "            continue\n",
    "\n",
    "    # í•„í„°ë§ ë¡œì§ (300ì ì´ìƒ, ì„ ì •ì  í‘œí˜„ ì œê±°, ì¤‘ë³µ URL ì œê±°)\n",
    "    filtered_data = []\n",
    "    for item in crawled_data:\n",
    "        if len(item[\"content\"]) >= 300:\n",
    "            if not any(word in item[\"content\"].lower() for word in [\"ì¶©ê²©\", \"í­ë¡œ\", \"clickbait\", \"ë…¼ë€ì˜\", \"ëŒ€ë°•\"]):\n",
    "                if item[\"url\"] not in [d.get(\"url\") for d in filtered_data]:\n",
    "                    filtered_data.append(item)\n",
    "\n",
    "    print(f\"âœ… ì›¹ í¬ë¡¤ë§ ì™„ë£Œ ({len(filtered_data)}ê°œ ë¬¸ì„œ)\")\n",
    "    return filtered_data\n",
    "\n",
    "print(\"âœ… ì›¹ í¬ë¡¤ë§ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e403121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Issue ë©”ëª¨ë¦¬ êµ¬ì¶• í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# step6. Issue ë©”ëª¨ë¦¬ êµ¬ì¶• (ì›¹ í¬ë¡¤ë§ ê²°ê³¼ë¥¼ RAGì— ì €ì¥)\n",
    "def build_issue_memory(keywords: List[str]):\n",
    "    # ... (Issue memory build logic remains the same)\n",
    "    crawled_data = crawl_web_content(keywords)\n",
    "    issue_docs = []\n",
    "    \n",
    "    for item in crawled_data:\n",
    "        doc = Document(\n",
    "            page_content=f\"[ì´ìŠˆ: {item['category']}] {item['title']}\\n\\n{item['content']}\",\n",
    "            metadata={\n",
    "                \"document_type\": \"issue\",\n",
    "                \"source\": item[\"source\"],\n",
    "                \"url\": item[\"url\"],\n",
    "                \"date\": item[\"date\"],\n",
    "                \"category\": item[\"category\"],\n",
    "                \"title\": item[\"title\"]\n",
    "            }\n",
    "        )\n",
    "        issue_docs.append(doc)\n",
    "    \n",
    "    if issue_docs:\n",
    "        issue_vectorstore = Chroma.from_documents(\n",
    "            documents=issue_docs,\n",
    "            embedding=embedding_model,\n",
    "            persist_directory=issue_embed_dir\n",
    "        )\n",
    "        print(f\"âœ… Issue ë©”ëª¨ë¦¬ êµ¬ì¶• ì™„ë£Œ ({len(issue_docs)}ê°œ ë¬¸ì„œ)\")\n",
    "        return issue_vectorstore\n",
    "    else:\n",
    "        print(\"âš ï¸ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "\n",
    "print(\"âœ… Issue ë©”ëª¨ë¦¬ êµ¬ì¶• í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n",
    "\n",
    "\n",
    "# ğŸ’¡ ì‹ ê·œ í•¨ìˆ˜ ì •ì˜: LLMì„ ì´ìš©í•œ ì¦ê±° ìš”ì•½\n",
    "def summarize_evidence_with_llm(docs: List[Document], query: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"ê²€ìƒ‰ëœ Document ëª©ë¡ì„ LLMì„ ì‚¬ìš©í•˜ì—¬ ìš”ì•½í•˜ê³  ì„¸ë¶€ ì •ë³´ì™€ ê²°í•©í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if not llm:\n",
    "        print(\"âš ï¸ LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "        return []\n",
    "\n",
    "    summarized_results = []\n",
    "    \n",
    "    summary_prompt_template = \"\"\"ë‹¹ì‹ ì€ AI ìœ¤ë¦¬ ë¦¬ìŠ¤í¬ ì§„ë‹¨ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ í•œêµ­ì–´ë¡œ 3ì¤„ ì´ë‚´ì˜ ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸ ìš”ì•½ì„ ì œê³µí•˜ì„¸ìš”.\n",
    "    ì´ ìš”ì•½ì€ 'AI ì„œë¹„ìŠ¤ {query}ì˜ ìœ¤ë¦¬ ë¦¬ìŠ¤í¬'ì— ëŒ€í•œ ê·¼ê±°ë¡œ ì‚¬ìš©ë  ê²ƒì…ë‹ˆë‹¤.\n",
    "    ---\n",
    "    ë¬¸ì„œ ì¶œì²˜: {source} ({document_type}) {chunk_info}\n",
    "    ë¬¸ì„œ ë‚´ìš©: {content}\n",
    "    ---\n",
    "    ìš”ì•½:\"\"\"\n",
    "    summary_prompt = PromptTemplate(template=summary_prompt_template, input_variables=[\"query\", \"source\", \"document_type\", \"content\", \"chunk_info\"])\n",
    "\n",
    "    for doc in docs:\n",
    "        content = doc.page_content\n",
    "        source = doc.metadata.get(\"source\", doc.metadata.get(\"url\", \"Unknown\"))\n",
    "        doc_type = doc.metadata.get(\"document_type\", \"Unknown\")\n",
    "        category = doc.metadata.get(\"category\", \"N/A\")\n",
    "\n",
    "        # ë¬¸ì„œ íƒ€ì…ì— ë”°ë¥¸ ì²­í¬ ì •ë³´ ì„¤ì •\n",
    "        if doc_type == \"baseline\":\n",
    "            chunk_info = f\"(í˜ì´ì§€ {doc.metadata.get('page', 'N/A')}ì˜ ë‚´ìš©)\"\n",
    "            score = 0.8\n",
    "        else: # issue\n",
    "            chunk_info = \"(ì›¹ ê¸°ì‚¬ ì›ë¬¸)\"\n",
    "            score = 0.2\n",
    "\n",
    "        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° ìš”ì•½ ìƒì„±\n",
    "        prompt_value = summary_prompt.invoke({\n",
    "            \"query\": query,\n",
    "            \"source\": source,\n",
    "            \"document_type\": doc_type,\n",
    "            \"content\": content,\n",
    "            \"chunk_info\": chunk_info\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            # LLM í˜¸ì¶œ\n",
    "            summary_response = llm.invoke(prompt_value.to_string())\n",
    "            summary = summary_response.content.strip()\n",
    "        except Exception as e:\n",
    "            summary = f\"LLM ìš”ì•½ ì‹¤íŒ¨. Error: {e}\"\n",
    "        \n",
    "        # Risk Assessor ì—ì´ì „íŠ¸ì— ì „ë‹¬í•  ìƒì„¸ êµ¬ì¡°\n",
    "        summarized_results.append({\n",
    "            \"category\": category,\n",
    "            \"document_type\": doc_type,\n",
    "            \"source\": source,\n",
    "            \"chunk_info\": chunk_info, # PDF í˜ì´ì§€ ë˜ëŠ” ì›¹ ê¸°ì‚¬ ì—¬ë¶€\n",
    "            \"score\": score,\n",
    "            \"summary\": summary, \n",
    "            \"content_excerpt\": content[:300] + \"...\", # ì›ë¬¸ ë‚´ìš©ì˜ ì¼ë¶€ (ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡)\n",
    "            \"full_content\": content # Risk Assessorì—ì„œ í•„ìš”í•  ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ì „ì²´ ì›ë¬¸ë„ ì „ë‹¬\n",
    "        })\n",
    "        \n",
    "    return summarized_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a131f2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì¦ê±° ìˆ˜ì§‘ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# step7. ì¦ê±° ìˆ˜ì§‘ í•¨ìˆ˜ ì •ì˜ (ê°€ì¤‘ì¹˜ 8:2 ì ìš©)\n",
    "def collect_evidence(service_profile: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    ì„œë¹„ìŠ¤ í”„ë¡œíŒŒì¼ ê¸°ë°˜ ì¦ê±° ìˆ˜ì§‘ (Baseline 0.8 : Issue 0.2)\n",
    "    - Risk Assessorì—ê²Œ ì „ë‹¬í•  ì¦ê±° ì†ŒìŠ¤ ëª©ë¡ ë° ê°€ì¤‘ì¹˜ ì ìˆ˜, ìš”ì•½ í¬í•¨\n",
    "    \"\"\"\n",
    "    \n",
    "    service_name = service_profile.get(\"service_name\", \"\")\n",
    "    risk_categories = service_profile.get(\"risk_categories\", [])\n",
    "    service_type = service_profile.get(\"service_type\", \"\")\n",
    "    \n",
    "    print(f\"\\nğŸ” ì¦ê±° ìˆ˜ì§‘ ì‹œì‘: {service_name}\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ êµ¬ì¶•\n",
    "    baseline_vectorstore = build_baseline_memory()\n",
    "    issue_vectorstore = build_issue_memory(risk_categories)\n",
    "    \n",
    "    evidence_results = {\n",
    "        \"query\": service_name,\n",
    "        \"weights\": {\"baseline\": 0.8, \"issue\": 0.2},\n",
    "        \"scores\": {},\n",
    "        \"baseline_sources\": [],\n",
    "        \"issue_sources\": []\n",
    "    }\n",
    "    \n",
    "    all_docs_to_summarize = []\n",
    "    \n",
    "    # 1. ê° ë¦¬ìŠ¤í¬ ì¹´í…Œê³ ë¦¬ë³„ ì¦ê±° ê²€ìƒ‰\n",
    "    for category in risk_categories:\n",
    "        \n",
    "        # Baseline ê²€ìƒ‰ ì¿¼ë¦¬ ê°•í™”\n",
    "        baseline_query = f\"{service_name} {category} ë¦¬ìŠ¤í¬ {service_type} (EU AI Act, OECD, UNESCO ìœ¤ë¦¬ ê¸°ì¤€)\"\n",
    "        issue_query = f\"ìµœì‹  ë‰´ìŠ¤ ë…¼ë¬¸ AI {service_name} {category} ë¬¸ì œ\"\n",
    "        \n",
    "        print(f\"\\n   ğŸ“Š {category.upper()} ë¦¬ìŠ¤í¬ ê²€ìƒ‰ ì¤‘...\")\n",
    "        \n",
    "        # Baseline ê²€ìƒ‰\n",
    "        baseline_docs = baseline_vectorstore.similarity_search(baseline_query, k=3)\n",
    "        \n",
    "        # Issue ê²€ìƒ‰\n",
    "        issue_docs = []\n",
    "        if issue_vectorstore:\n",
    "            issue_docs = issue_vectorstore.similarity_search(issue_query, k=2)\n",
    "            \n",
    "        # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ìš”ì•½ ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (ì¹´í…Œê³ ë¦¬ ë©”íƒ€ë°ì´í„° ë¶€ì—¬)\n",
    "        for doc in baseline_docs:\n",
    "            doc.metadata['category'] = category\n",
    "            all_docs_to_summarize.append(doc)\n",
    "        for doc in issue_docs:\n",
    "            doc.metadata['category'] = category\n",
    "            all_docs_to_summarize.append(doc)\n",
    "            \n",
    "        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ì°¸ê³ ìš©)\n",
    "        baseline_weight = 0.8\n",
    "        issue_weight = 0.2 if issue_docs else 0.0\n",
    "        total_score = (len(baseline_docs) > 0) * baseline_weight + (len(issue_docs) > 0) * issue_weight\n",
    "        evidence_results[\"scores\"][category] = total_score\n",
    "        \n",
    "        print(f\" - ê²€ìƒ‰ëœ Baseline ì²­í¬: {len(baseline_docs)}ê°œ\")\n",
    "        print(f\" - ê²€ìƒ‰ëœ Issue ë¬¸ì„œ: {len(issue_docs)}ê°œ\")\n",
    "\n",
    "    print(\"\\nğŸ“ ê²€ìƒ‰ëœ ì¦ê±°ë“¤ì„ LLMì„ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ ì¤‘...\")\n",
    "    \n",
    "    # 2. í†µí•© ìš”ì•½ ë° ë°ì´í„° êµ¬ì¡°í™”\n",
    "    summarized_evidences = summarize_evidence_with_llm(all_docs_to_summarize, service_name)\n",
    "    \n",
    "    # 3. ìµœì¢… ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    for evidence in summarized_evidences:\n",
    "        if evidence['document_type'] == 'baseline':\n",
    "            evidence_results[\"baseline_sources\"].append(evidence)\n",
    "        elif evidence['document_type'] == 'issue':\n",
    "            evidence_results[\"issue_sources\"].append(evidence)\n",
    "    \n",
    "    print(f\"\\nâœ… ì¦ê±° ìˆ˜ì§‘ ë° ìš”ì•½ ì™„ë£Œ!\")\n",
    "    return evidence_results\n",
    "\n",
    "print(\"âœ… ì¦ê±° ìˆ˜ì§‘ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a4c0121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ” Evidence Collector ì‹œì‘...\n",
      "============================================================\n",
      "\n",
      "ğŸ“ ë¶„ì„í•  ì„œë¹„ìŠ¤: ì±„ìš© ì§€ì›ìì˜ ì´ë ¥ì„œë¥¼ AIë¡œ ë¶„ì„í•˜ì—¬ ì í•©í•œ í›„ë³´ìë¥¼ ì¶”ì²œí•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤....\n",
      "\n",
      "ğŸ” ì¦ê±° ìˆ˜ì§‘ ì‹œì‘: ì´ë ¥ì„œ ë¶„ì„ ì¶”ì²œ ì‹œìŠ¤í…œ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… EU_AI_Act.pdf ë¡œë“œ ì™„ë£Œ\n",
      "âœ… OECD_Privacy_2024.pdf ë¡œë“œ ì™„ë£Œ\n",
      "âœ… UNESCO_Ethics_2021.pdf ë¡œë“œ ì™„ë£Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Baseline ë©”ëª¨ë¦¬ êµ¬ì¶• ì™„ë£Œ (2075ê°œ ì²­í¬)\n",
      " Â  Â - Tavily ê²€ìƒ‰ ì¤‘: AI bias í¸í–¥ì„± ë¬¸ì œ...\n",
      " Â  Â - Tavily ê²€ìƒ‰ ì¤‘: AI privacy í¸í–¥ì„± ë¬¸ì œ...\n",
      " Â  Â - Tavily ê²€ìƒ‰ ì¤‘: AI transparency ê°œì¸ì •ë³´ë³´í˜¸...\n",
      " Â  Â - Tavily ê²€ìƒ‰ ì¤‘: AI privacy ìœ¤ë¦¬ ì´ìŠˆ...\n",
      " Â  Â - Tavily ê²€ìƒ‰ ì¤‘: AI bias ìœ¤ë¦¬ ì´ìŠˆ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì›¹ í¬ë¡¤ë§ ì™„ë£Œ (23ê°œ ë¬¸ì„œ)\n",
      "âœ… Issue ë©”ëª¨ë¦¬ êµ¬ì¶• ì™„ë£Œ (23ê°œ ë¬¸ì„œ)\n",
      "\n",
      "   ğŸ“Š BIAS ë¦¬ìŠ¤í¬ ê²€ìƒ‰ ì¤‘...\n",
      " - ê²€ìƒ‰ëœ Baseline ì²­í¬: 3ê°œ\n",
      " - ê²€ìƒ‰ëœ Issue ë¬¸ì„œ: 2ê°œ\n",
      "\n",
      "   ğŸ“Š PRIVACY ë¦¬ìŠ¤í¬ ê²€ìƒ‰ ì¤‘...\n",
      " - ê²€ìƒ‰ëœ Baseline ì²­í¬: 3ê°œ\n",
      " - ê²€ìƒ‰ëœ Issue ë¬¸ì„œ: 2ê°œ\n",
      "\n",
      "   ğŸ“Š TRANSPARENCY ë¦¬ìŠ¤í¬ ê²€ìƒ‰ ì¤‘...\n",
      " - ê²€ìƒ‰ëœ Baseline ì²­í¬: 3ê°œ\n",
      " - ê²€ìƒ‰ëœ Issue ë¬¸ì„œ: 2ê°œ\n",
      "\n",
      "ğŸ“ ê²€ìƒ‰ëœ ì¦ê±°ë“¤ì„ LLMì„ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ ì¤‘...\n",
      "\n",
      "âœ… ì¦ê±° ìˆ˜ì§‘ ë° ìš”ì•½ ì™„ë£Œ!\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š ì¦ê±° ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½ ë° ë‹¤ìŒ ì—ì´ì „íŠ¸ ì „ë‹¬ ë‚´ìš©\n",
      "============================================================\n",
      "  - ì„œë¹„ìŠ¤ëª…: ì´ë ¥ì„œ ë¶„ì„ ì¶”ì²œ ì‹œìŠ¤í…œ\n",
      "  - ê°€ì¤‘ì¹˜: Baseline 0.8 : Issue 0.2\n",
      "  - ìˆ˜ì§‘ëœ ë¦¬ìŠ¤í¬ ì¹´í…Œê³ ë¦¬: ['bias', 'privacy', 'transparency']\n",
      "\n",
      "------------------------------------------------------------\n",
      "ğŸš¨ Risk Assessorì— ì „ë‹¬ë˜ëŠ” ë°ì´í„° êµ¬ì¡° ìš”ì•½ (ì²« ë²ˆì§¸ ì¦ê±° ì˜ˆì‹œ)\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Baseline ê·¼ê±° ì˜ˆì‹œ]\n",
      "  - ë¦¬ìŠ¤í¬ ì¹´í…Œê³ ë¦¬: BIAS\n",
      "  - ì¶œì²˜: UNESCO_Ethics_2021.pdf (í˜ì´ì§€ 40ì˜ ë‚´ìš©)\n",
      "  - **ìš”ì•½ (í•µì‹¬ ê·¼ê±°):** AI ì„œë¹„ìŠ¤ ì´ë ¥ì„œ ë¶„ì„ ì¶”ì²œ ì‹œìŠ¤í…œì€ UNESCOì˜ ì •ì±… ê¶Œê³ ì‚¬í•­ì— ë”°ë¼ ìœ¤ë¦¬ì  ê¸°ì¤€ì„ ì¤€ìˆ˜í•´ì•¼ í•˜ë©°, ì´ë¥¼ ìœ„í•´ ê²½í—˜ ê³µìœ  ë©”ì»¤ë‹ˆì¦˜ê³¼ AI ê·œì œ ìƒŒë“œë°•ìŠ¤ê°€ í•„ìš”í•˜ë‹¤. ì´ëŸ¬í•œ ë„êµ¬ë“¤ì€ AI ê´€ë ¨ ì£¼ì²´ë“¤ì´ ìœ¤ë¦¬ ë¦¬ìŠ¤í¬ë¥¼ í‰ê°€í•˜ê³  ê°œì„ í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•œë‹¤. ë”°ë¼ì„œ ì‹œìŠ¤í…œì˜ ì„¤ê³„ì™€ ìš´ì˜ì—ì„œ ìœ¤ë¦¬ì  ê³ ë ¤ê°€ í•„ìˆ˜ì ì´ë‹¤.\n",
      "  - ì›ë¬¸ ë‚´ìš© (ì¼ë¶€): across UNESCOâ€™s areas of competence, an experience-\n",
      "sharing mechanism, AI regulatory sandboxes, and an \n",
      "assessment guide for all AI actors to evaluate their \n",
      "adherence to policy recommendations mentioned in \n",
      "this document....\n",
      "\n",
      "[Issue ê·¼ê±° ì˜ˆì‹œ]\n",
      "  - ë¦¬ìŠ¤í¬ ì¹´í…Œê³ ë¦¬: BIAS\n",
      "  - ì¶œì²˜: https://dailyan.com/news/article.html?no=731402 (N/A) (ì›¹ ê¸°ì‚¬ ì›ë¬¸)\n",
      "  - **ìš”ì•½ (ì‚¬íšŒì  ë°˜ì‘):** AI ì„œë¹„ìŠ¤ ì´ë ¥ì„œ ë¶„ì„ ì¶”ì²œ ì‹œìŠ¤í…œì€ í¸í–¥ëœ í•™ìŠµ ë°ì´í„°ë¡œ ì¸í•´ ì°¨ë³„ì  ê²°ê³¼ë¥¼ ì´ˆë˜í•  ìœ„í—˜ì´ ìˆìœ¼ë©°, ì´ëŠ” ì‚¬íšŒì  ë¶ˆí‰ë“±ì„ ì‹¬í™”ì‹œí‚¬ ìˆ˜ ìˆë‹¤. ë˜í•œ, AIì˜ ê²°ì •ì— ëŒ€í•œ ì±…ì„ ì†Œì¬ê°€ ë¶ˆëª…í™•í•˜ì—¬ ë²•ì  ë° ìœ¤ë¦¬ì  í˜¼ë€ì„ ì•¼ê¸°í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ìœ¤ë¦¬ì  ê³ ë ¤ì™€ íˆ¬ëª…ì„±ì„ ê°•í™”í•˜ê³ , ëª…í™•í•œ ê¸°ì¤€ì„ ë§ˆë ¨í•˜ëŠ” ê²ƒì´ í•„ìˆ˜ì ì´ë‹¤.\n",
      "  - ì›ë¬¸ ë‚´ìš© (ì¼ë¶€): [ì´ìŠˆ: issue] AI ê°œë°œì˜ ìœ¤ë¦¬ì  ë¬¸ì œì  ì‹¬í™”: ì±…ì„ ì†Œì¬ì™€ í¸í–¥ì„± ë…¼ë€ í™•ëŒ€\n",
      "\n",
      "í˜ì´ìŠ¤ë¶\n",
      " ì—‘ìŠ¤\n",
      " ì¹´ì¹´ì˜¤í†¡\n",
      " ë„¤ì´ë²„ë¸”ë¡œê·¸\n",
      "\n",
      "### ê¸‰ì†í•œ AI ë°œì „ ì†ì—ì„œ ìœ¤ë¦¬ì  ë¬¸ì œê°€ ì‚¬íšŒì  ë…¼ìŸìœ¼ë¡œ í™•ëŒ€ë˜ëŠ” í˜„í™© ë¶„ì„\n",
      "\n",
      "ë°ì¼ë¦¬ì—°í•© (SNSJTV. íƒ€ì„ì¦ˆM) ê¹€ë¯¼ì œ ê¸°ì | ìµœê·¼ ê¸‰ì†ë„ë¡œ ë°œì „í•˜ëŠ” ì¸ê³µì§€ëŠ¥(AI) ê¸°ìˆ ì€ í¸ë¦¬í•¨ê³¼ íš¨ìœ¨ì„±ì„ ì œê³µí•˜ì§€ë§Œ ë™ì‹œì— ì‹¬ê°í•œ ìœ¤ë¦¬ì  ë¬¸ì œë¥¼ ì•¼ê¸°í•œë‹¤ëŠ” ìš°ë ¤ê°€ ì»¤ì§€ê³  ìˆë‹¤.\n",
      "\n",
      "AI ê°œë°œ ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” í¸í–¥ì„± ë¬¸ì œëŠ” ì‹¬ê°í•œ ì‚¬íšŒì  ë¶ˆí‰ë“±ì„ ì´ˆë˜í•  ìˆ˜ ìˆë‹¤. AI ì•Œê³ ë¦¬ì¦˜ì€ í•™ìŠµ ë°ì´í„°ì— ì˜ì¡´í•˜ëŠ”ë°, ì´ ...\n",
      "\n",
      "------------------------------------------------------------\n",
      "ğŸ”— ìµœì¢…ì ìœ¼ë¡œ 15ê°œì˜ ìƒì„¸ ì¦ê±°ê°€ Risk Assessorë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# step8. í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ” Evidence Collector ì‹œì‘...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Service Profilerì—ì„œ ë°›ì€ ê²°ê³¼ (ì˜ˆì‹œ)\n",
    "test_service_profile = {\n",
    "    \"service_name\": \"ì´ë ¥ì„œ ë¶„ì„ ì¶”ì²œ ì‹œìŠ¤í…œ\",\n",
    "    \"service_type\": \"recruitment system\", \n",
    "    \"description\": \"ì±„ìš© ì§€ì›ìì˜ ì´ë ¥ì„œë¥¼ AIë¡œ ë¶„ì„í•˜ì—¬ ì í•©í•œ í›„ë³´ìë¥¼ ì¶”ì²œí•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\",\n",
    "    \"risk_categories\": [\"bias\", \"privacy\", \"transparency\"]\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“ ë¶„ì„í•  ì„œë¹„ìŠ¤: {test_service_profile['description']}...\")\n",
    "\n",
    "# ì¦ê±° ìˆ˜ì§‘ ì‹¤í–‰\n",
    "evidence_result = collect_evidence(test_service_profile)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š ì¦ê±° ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½ ë° ë‹¤ìŒ ì—ì´ì „íŠ¸ ì „ë‹¬ ë‚´ìš©\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  - ì„œë¹„ìŠ¤ëª…: {evidence_result['query']}\")\n",
    "print(f\"  - ê°€ì¤‘ì¹˜: Baseline {evidence_result['weights']['baseline']} : Issue {evidence_result['weights']['issue']}\")\n",
    "print(f\"  - ìˆ˜ì§‘ëœ ë¦¬ìŠ¤í¬ ì¹´í…Œê³ ë¦¬: {list(evidence_result['scores'].keys())}\")\n",
    "\n",
    "print(\"\\n------------------------------------------------------------\")\n",
    "print(\"ğŸš¨ Risk Assessorì— ì „ë‹¬ë˜ëŠ” ë°ì´í„° êµ¬ì¡° ìš”ì•½ (ì²« ë²ˆì§¸ ì¦ê±° ì˜ˆì‹œ)\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "\n",
    "# ì „ë‹¬ë  ë°ì´í„° êµ¬ì¡° ì˜ˆì‹œ ì¶œë ¥ (ì²« ë²ˆì§¸ Baseline ì†ŒìŠ¤ì™€ ì²« ë²ˆì§¸ Issue ì†ŒìŠ¤)\n",
    "if evidence_result['baseline_sources']:\n",
    "    b_src = evidence_result['baseline_sources'][0]\n",
    "    print(\"\\n[Baseline ê·¼ê±° ì˜ˆì‹œ]\")\n",
    "    print(f\"  - ë¦¬ìŠ¤í¬ ì¹´í…Œê³ ë¦¬: {b_src['category'].upper()}\")\n",
    "    print(f\"  - ì¶œì²˜: {b_src['source']} {b_src['chunk_info']}\")\n",
    "    print(f\"  - **ìš”ì•½ (í•µì‹¬ ê·¼ê±°):** {b_src['summary']}\")\n",
    "    print(f\"  - ì›ë¬¸ ë‚´ìš© (ì¼ë¶€): {b_src['content_excerpt']}\")\n",
    "\n",
    "if evidence_result['issue_sources']:\n",
    "    i_src = evidence_result['issue_sources'][0]\n",
    "    print(\"\\n[Issue ê·¼ê±° ì˜ˆì‹œ]\")\n",
    "    print(f\"  - ë¦¬ìŠ¤í¬ ì¹´í…Œê³ ë¦¬: {i_src['category'].upper()}\")\n",
    "    print(f\"  - ì¶œì²˜: {i_src['source']} ({i_src.get('url', 'N/A')}) {i_src['chunk_info']}\")\n",
    "    print(f\"  - **ìš”ì•½ (ì‚¬íšŒì  ë°˜ì‘):** {i_src['summary']}\")\n",
    "    print(f\"  - ì›ë¬¸ ë‚´ìš© (ì¼ë¶€): {i_src['content_excerpt']}\")\n",
    "\n",
    "print(\"\\n------------------------------------------------------------\")\n",
    "print(f\"ğŸ”— ìµœì¢…ì ìœ¼ë¡œ {len(evidence_result['baseline_sources']) + len(evidence_result['issue_sources'])}ê°œì˜ ìƒì„¸ ì¦ê±°ê°€ Risk Assessorë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-JIaWGMA_-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
