#!/usr/bin/env python3
"""
Risk Assessor Agent - EU AI Act ê¸°ì¤€ ìœ„í—˜ ë“±ê¸‰ ì‚°ì •
"""

import os
import sys
import json
import re
from typing import Dict, Any, List

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("âš ï¸ dotenv ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

# LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
try:
    from langchain_openai import ChatOpenAI
    from langchain.prompts import ChatPromptTemplate
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("âš ï¸ LangChainì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ LLM ê¸°ë°˜ í‰ê°€ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")


def risk_assessor_execute():
    """Risk Assessor ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "=" * 60)
    print("âš–ï¸ Risk Assessor ì‹œì‘...")
    print("=" * 60)

    sys.path.append("..")
    from state_manager import load_state, save_state, update_status

    state = load_state()

    # Evidence Collector ì™„ë£Œ ì—¬ë¶€ í™•ì¸
    if state.get("status", {}).get("evidence_collector") != "completed":
        print("âŒ Evidence Collectorê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Risk Assessorë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        update_status(state, "risk_assessor", "skipped")
        save_state(state)
        return state

    # Evidence Data í™•ì¸
    evidence_data = state.get("evidence_data", {})
    if not evidence_data:
        print("âŒ Evidence Dataê°€ ì—†ìŠµë‹ˆë‹¤. Risk Assessorë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        update_status(state, "risk_assessor", "failed")
        save_state(state)
        return state

    # í‰ê°€ ìˆ˜í–‰
    updated_state = assess_risk_and_update_state(state)

    # ê²°ê³¼ ì €ì¥
    save_state(updated_state)
    update_status(updated_state, "risk_assessor", "completed")
    
    print("âœ… ë¦¬ìŠ¤í¬ í‰ê°€ ì™„ë£Œ ë° ìƒíƒœ ì €ì¥ ì™„ë£Œ!")
    return updated_state


def assess_risk_and_update_state(state: Dict[str, Any]) -> Dict[str, Any]:
    """ì¦ê±° ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¦¬ìŠ¤í¬ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ê³  stateì— ë°˜ì˜í•©ë‹ˆë‹¤."""

    evidence_data = state.get("evidence_data", {})
    service_name = evidence_data.get("query", "AI ì„œë¹„ìŠ¤")
    risk_categories = list(evidence_data.get("scores", {}).keys())

    # Risk categoriesê°€ ë¹„ì–´ìˆìœ¼ë©´ service_profileì—ì„œ ê°€ì ¸ì˜¤ê¸°
    if not risk_categories:
        service_profile = state.get("service_profile", {})
        risk_categories = service_profile.get("diagnosed_risk_categories", ["bias", "privacy", "transparency"])

    print(f"\nâš–ï¸ Risk Assessor: {service_name}ì˜ ë¦¬ìŠ¤í¬ í‰ê°€ ì‹œì‘")
    assessed_risks = []

    # LLM ì´ˆê¸°í™”
    llm = None
    if LANGCHAIN_AVAILABLE:
        try:
            llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2)
        except Exception as e:
            print(f"âš ï¸ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    # ê° ë¦¬ìŠ¤í¬ ì¹´í…Œê³ ë¦¬ë³„ í‰ê°€
    for category in risk_categories:
        print(f"\n   ğŸ” {category.upper()} ë¦¬ìŠ¤í¬ í‰ê°€ ì¤‘...")

        # Baseline ê·¼ê±° ìˆ˜ì§‘
        baseline_summaries = []
        for src in evidence_data.get("baseline_sources", []):
            if src.get("category") == category:
                baseline_summaries.append(
                    f"- [Baseline] ì¶œì²˜: {src['source']} {src['chunk_info']}. ìš”ì•½: {src['summary']}"
                )

        # Issue ê·¼ê±° ìˆ˜ì§‘
        issue_summaries = []
        for src in evidence_data.get("issue_sources", []):
            if src.get("category") == category:
                issue_summaries.append(
                    f"- [Issue] ì¶œì²˜: {src['source']}. ìš”ì•½: {src['summary']}"
                )

        baseline_text = "\n".join(baseline_summaries) if baseline_summaries else "ì¦ê±° ì—†ìŒ"
        issue_text = "\n".join(issue_summaries) if issue_summaries else "ì¦ê±° ì—†ìŒ"

        # LLM ê¸°ë°˜ í‰ê°€
        if llm:
            try:
                assessment_result = evaluate_with_llm(
                    llm, service_name, category, baseline_text, issue_text
                )
                assessed_risks.append(assessment_result)
                print(f"     âœ… í‰ê°€ ì™„ë£Œ: {category.upper()} -> {assessment_result.get('risk_level', 'Unknown')}")
            except Exception as e:
                print(f"     âš ï¸ LLM í‰ê°€ ì‹¤íŒ¨ ({category}): {e}")
                result = create_simple_assessment(category, service_name)
                assessed_risks.append(result)
        else:
            # ê·œì¹™ ê¸°ë°˜ í‰ê°€
            result = create_simple_assessment(category, service_name)
            assessed_risks.append(result)
            print(f"     âœ… í‰ê°€ ì™„ë£Œ: {category.upper()} -> {result['risk_level']}")

    # âœ… í•µì‹¬: Stateì— í‰ê°€ ê²°ê³¼ ì €ì¥
    final_assessment = {
        "service_name": service_name,
        "assessed_risks": assessed_risks
    }

    state["assessment_result"] = final_assessment
    state["assessment_status"] = "success"

    print(f"\nâœ… Risk Assessor í‰ê°€ ì™„ë£Œ! (ì´ {len(assessed_risks)}ê°œ ë¦¬ìŠ¤í¬ í‰ê°€ë¨)")
    return state


def evaluate_with_llm(llm, service_name, category, baseline_text, issue_text):
    """LLMì„ ì‚¬ìš©í•œ ë¦¬ìŠ¤í¬ í‰ê°€ - JSON ê°•ì œ íŒŒì‹±"""
    
    from langchain_core.output_parsers import JsonOutputParser
    from pydantic import BaseModel, Field
    
    # âœ… Pydantic ëª¨ë¸ë¡œ JSON êµ¬ì¡° ê°•ì œ
    class RiskAssessment(BaseModel):
        category: str = Field(description="ë¦¬ìŠ¤í¬ ì¹´í…Œê³ ë¦¬")
        risk_level: str = Field(description="High, Limited, Minimal ì¤‘ í•˜ë‚˜")
        assessment_summary: str = Field(description="í‰ê°€ ê·¼ê±° 3ì¤„ ìš”ì•½")
        recommendation_focus: str = Field(description="êµ¬ì²´ì ì¸ ê°œì„  ë°©í–¥")
    
    parser = JsonOutputParser(pydantic_object=RiskAssessment)
    
    # âœ… JSON í˜•ì‹ì„ ê°•ì œí•˜ëŠ” í”„ë¡¬í”„íŠ¸
    ASSESSMENT_PROMPT = ChatPromptTemplate.from_messages([
        ("system", 
         "ë‹¹ì‹ ì€ AI ìœ¤ë¦¬ ë¦¬ìŠ¤í¬ë¥¼ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
         "EU AI Act, OECD, UNESCO ê¸°ì¤€ì„ ë°”íƒ•ìœ¼ë¡œ ë¦¬ìŠ¤í¬ë¥¼ í‰ê°€í•˜ì„¸ìš”.\n\n"
         "{format_instructions}"
        ),
        ("human", 
         "ì„œë¹„ìŠ¤ëª…: {service_name}\n"
         "ë¦¬ìŠ¤í¬ ì¹´í…Œê³ ë¦¬: {category}\n\n"
         "Baseline ê·¼ê±°:\n{baseline_summaries}\n\n"
         "Issue ê·¼ê±°:\n{issue_summaries}\n\n"
         "ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”."
        )
    ])

    chain = ASSESSMENT_PROMPT | llm

    response = chain.invoke({
        "service_name": service_name,
        "category": category,
        "baseline_summaries": baseline_text,
        "issue_summaries": issue_text
    })

    # âœ… ê°•ì œ JSON íŒŒì‹±
    try:
        # JSON ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ
        content = response.content
        json_match = re.search(r'```json\s*(\{.*?\})\s*```', content, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group(1))
        else:
            # JSON ë¸”ë¡ ì—†ìœ¼ë©´ ì „ì²´ì—ì„œ ì°¾ê¸°
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group(0))
            else:
                raise ValueError("JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        result["category"] = category
        return result
        
    except Exception as e:
        print(f"     âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨, í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ ì‹œë„: {e}")
        # í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ
        content = response.content
        
        # ë¦¬ìŠ¤í¬ ë ˆë²¨ ì¶”ì¶œ
        if "High" in content:
            risk_level = "High"
        elif "Limited" in content:
            risk_level = "Limited"
        else:
            risk_level = "Minimal"
        
        # ì²« 500ìë¥¼ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©
        summary = content[:500].replace("\n", " ")
        
        return {
            "category": category,
            "risk_level": risk_level,
            "assessment_summary": summary,
            "recommendation_focus": f"{category} ê´€ë ¨ ê°œì„  ë°©ì•ˆ ìˆ˜ë¦½ í•„ìš”"
        }


def create_simple_assessment(category: str, service_name: str) -> Dict[str, Any]:
    """ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ë¦¬ìŠ¤í¬ í‰ê°€"""
    risk_mapping = {
        "bias": "High",
        "privacy": "High",
        "transparency": "Limited",
        "accountability": "Limited",
        "safety": "Minimal",
        "security": "Limited"
    }

    level = risk_mapping.get(category, "Limited")

    return {
        "category": category,
        "risk_level": level,
        "assessment_summary": f"{service_name}ì˜ {category} ë¦¬ìŠ¤í¬ëŠ” EU AI Act ê¸°ì¤€ìœ¼ë¡œ {level} ìˆ˜ì¤€ìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤.",
        "recommendation_focus": f"{category} ê´€ë ¨ ê°œì„  ë°©ì•ˆ ìˆ˜ë¦½ í•„ìš”"
    }


if __name__ == "__main__":
    risk_assessor_execute()